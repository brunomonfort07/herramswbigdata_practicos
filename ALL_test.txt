Roles en el Mundo
    de Datos
                ¿QUÉ ES UN ROL
UN ROL SE REFIERE A LAS RESPONSABILIDADES, TAREAS Y FUNCIONES
ASIGNADAS A UNA PERSONA DENTRO DE UNA ORGANIZACIÓN. LOS ROLES EN
EL ÁMBITO LABORAL PUEDEN SER ESPECÍFICOS Y ESTAR DEFINIDOS POR UN
TÍTULO    O   POSICIÓN,    COMO    "GERENTE",    "DESARROLLADOR",
"ADMINISTRADOR", ENTRE OTROS.

DENTRO DE UNA ORGANIZACIÓN UNA PERSONA PUEDE TENER UNO O MÁS
ROLES. EN ORGANIZACIONES QUE TRABAJAN CON DATOS, HAY CIERTOS
ROLES DEFINIDOS QUE SE REPITEN EN MÁS DE UNA ORGANIZACIÓN Y
APARECEN EN LAS DISTINTAS OFERTAS LABORALES CON CIERTOS
REQUERIMIENTOS EN COMÚN.
               ¿QUÉ ES UN ROL
EN OCASIONES EN ESTOS ROLES ENCONTRAMOS UN SOLAPAMIENTO DE
TAREAS A EJECUTAR DENTRO DE LA ORGANIZACIÓN. PARA PODER
DEFINIRLOS VAMOS A OBSERVAR EL PROCESO DE USO DE LOS DATOS DESDE
SU ORIGEN HASTA EL USUARIO FINAL, PARA EXPLICAR EN QUE ETAPA
DESEMPEÑA CADA ROL SUS DISTINTAS FUNCIONES.

LA ESTRUCTURA DE ROLES PUEDE VARIAR SEGÚN LA ORGANIZACIÓN Y SUS
NECESIDADES ESPECÍFICAS, PERO ESTOS ROLES PROPORCIONAN UNA BASE
SÓLIDA PARA GESTIONAR EFICAZMENTE GRANDES VOLÚMENES DE DATOS.
                  ROLES EN EL PROCESO
                                        DE DATOS



                  Ingenieros                        Científicos                   Consumidores
                   de Datos                         de Datos                         Finales



Administradores                Análisita de datos                 Ingenieros de
 de bases de                                                         Machine
    Datos                                                            Learing
ADMINISTRADORES BASES DE
        DATOS
INGENIEROS DE DATOS
INGENIEROS DE DATOS
INGENIEROS DE DATOS
INGENIEROS DE DATOS
ANALISTA DE DATOS
ANALISTA DE DATOS
ANALISTA DE DATOS
CIENTÍFICO DE DATOS
INGENIEROS DE MACHINE
       LEARNING
CONSUMIDORES FINALES DE
        DATOS
  03 -
Datalakes
         ¿QUÉ ES BIG DATA?
BIG DATA SON LOS DATOS QUE EXCEDEN LA CAPACIDAD DE
PROCESAMIENTO     DE   LOS  SISTEMAS     DE BASES   DE  DATOS
CONVENCIONALES. LOS DATOS SON DEMASIADO GRANDES, SE MUEVEN
DEMASIADO RÁPIDO O NO SE AJUSTAN A LAS RESTRICCIONES DE LAS
ARQUITECTURAS DE BASE DE DATOS. PARA OBTENER VALOR DE ESTOS
DATOS, SE DEBE ELEGIR UNA FORMA ALTERNATIVA DE PROCESARLOS.
                      EDD DUMBILL, O'REILLY
 ¿QUÉ HACE QUE LA DATA SEA BIG
             LAS 5 V.
            DATA?
VALOR. LA INFORMACIÓN ES DE VALOR E INTERÉS DE LA ORGANIZACIÓN.
VOLUMEN. LA INFORMACIÓN ES DE UN GRAN VOLÚMEN, TAL QUE NO
PUEDE SER GESTIONADA POR UN SISTEMA DE BASE DE DATOS
CONVENCIONAL.
VELOCIDAD. LOS DATOS SE GENERAN A DISTINTAS VELOCIDADES.
ALGUNOS EN TIEMPO REAL, OTROS DE FORMA DIARIA, SEMANAL,
MENSUAL O ANUAL.
 ¿QUÉ HACE QUE LA DATA SEA BIG
            DATA?
             LAS 5 V.

VARIEDAD. LOS DATOS SON DE VARIADOS ORÍGENES, SOBRE DISTINTOS
DOMINIOS DEL NEGOCIO, Y VIENEN EN VARIADOS FORMATOS. (CLAVE
VALOR, JSON, TABULARES, XML, TEXTO, FOTOS, CSV, ETC.). SON
ESTRUCTURADOS Y NO ESTRUCTURADOS.
VERACIDAD. LOS DATOS DEBEN REPRESENTAR LA REALIDAD DE LA
ORGANIZACIÓN. SE DEBE CUIDAR ANTES DE INGESTARLOS, AL
PROCESARLOS, Y AL ENTREGARLOS QUE ESTOS SEAN VERACES.
¿A QUÉ PROBLEMA
NOS
ENFRENTAMOS?
  01     02    03
                          MÁS FUENTES DE
  DATOS DISPERSOS                                 NECESIDADES
                              DATOS

 Las organizaciones       Además pueden
 tienen una o varias    contar con sensores    Las organizaciones
 aplicaciones. Esas     (iOT), e información    necesitan juntar y
 aplicaciones tienen        externa a la          compilar esos
     uno o varios       organización. Redes    grandes volúmenes
 servicios que usan         sociales que       de datos para poder
  diversas bases de      podemos scrapear,     disponer de ellos a
 datos, con distintos    datos públicos que     tiempos y costos
     paradigmas.           podemos tomar           razonables.
    (Relacionales,        mediante apis u
Documentos, Grafos)        otros métodos.
         ¿CUÁL ES LA SOLUCIÓN?
                        EL DATALAKE.

UNA LAGUNA DE DATOS (DATALAKE) ES UN REPOSITORIO CENTRALIZADO
QUE PERMITE ALMACENAR TODOS LOS DATOS ESTRUCTURADOS Y NO
ESTRUCTURADOS A CUALQUIER ESCALA.
                                ¿CÓMO FUNCIONA EL
                                                                                                                         Los datos que tienen que

                                   DATALAKE?
                           Procesamos los datos
                        Los datos son procesados.
                                                                             Los usuarios finales
                                                                             pueden ver datos de
                                                                               diversos orígenes
                                                                                                                        estar disponibles por cierto
                                                                                                                        período de tiempo pueden
                                                                                                                        ser guardados en regiones
                         Se valida la integridad de                       compilados según la lógica                    donde luego son borrados.
                          los mismos y se les da                            del negocio, para sacar                       Los datos originales se
                               forma para su                                 valor de los mismos,                        guardan como respaldo y
                         almacenamiento. Se deja                           contando con información                     son accedidos en caso de
                         en la región (refined) listo                         agregada con gran                            tener que reprocesar
                             para ser utilizado.                              Segranularidad.
                                                                                   visualizan                                  Se guarda un
                               Se refinan
                                                                                los datos                                        respaldo


   Se ingestan los                                Se utilizan los datos                                Se consumen
        datos                                                                                            los datos
    Definimos regiones                              Los datos vuelven a la                          Los datos pueden ser
   Una de esas regiones                                   organización                              consumidos por otros
(landing) es utilizada para                           Se generan nuevos                          sistemas, no solo usuarios.
  la ingesta de los datos                         procesos donde los datos                       Se pueden volcar los datos
   crudos (En su formato                                 son cruzados y                            procesados a bases de
          original)                                 enriquecidos junto con                        datos, y disponibilizarlos
                                                  otros datos refinados, para                    para consumirlos mediante
                                                     ser consumidos por la                                  APIs
                                                          organización.
COMPONENTES DEL DATALAKE
    COMPONENTES DEL DATALAKE
                                       Visualización y
                                          Consumo
Fuentes
de Datos   DW
                      Almacenamiento




                      Procesamiento
                ETL


                        Modelado y
                        Consultas
04 - Sistemas de archivos
  disribuidos y buckets
    COMPONENTES DEL DATALAKE
                              Usted    Visualización y
                                          Consumo
Fuentes                       está
de Datos   DW                 aquí
                      Almacenamiento




                      Procesamiento
                ETL


                        Modelado y
                        Consultas
        ¿QUÉ ES UN SISTEMA DE ARCHIVOS
UN                   DISTRIBUDIO?
   SISTEMA DE ARCHIVOS DISTRIBUIDO ES UN TIPO DE SISTEMA  DE
ALMACENAMIENTO DE DATOS EN EL QUE LOS ARCHIVOS Y LOS
RECURSOS DE ALMACENAMIENTO SE DISTRIBUYEN EN MÚLTIPLES
NODOS O SERVIDORES INTERCONECTADOS EN UNA RED. ESTOS
SISTEMAS PERMITEN QUE VARIOS USUARIOS O APLICACIONES ACCEDAN
Y COMPARTAN ARCHIVOS A TRAVÉS DE LA RED DE MANERA
TRANSPARENTE, COMO SI TODOS LOS DATOS ESTUVIERAN EN UNA
UBICACIÓN FÍSICA CENTRALIZADA.
   CARACTERÍSTICAS SISTEMAS DE ARCHIVOS
LOS DATOS SE ALMACENAN Y DISTRIBUYEN ENTRE VARIOS NODOS EN LA
                     DISTRIBUDIOS
RED PERMITIENDO UN MEJOR RENDIMIENTO Y ESCALABILIDAD, YA QUE
EL ACCESO A LOS DATOS SE PUEDE DISTRIBUIR ENTRE VARIOS
SERVIDORES.

LOS DATOS SE ALMACENAN EN MÚLTIPLES UBICACIONES, PERO LOS
USUARIOS Y LAS APLICACIONES PUEDEN ACCEDEN A ELLOS COMO SI
ESTUVIERAN EN UN ÚNICO SISTEMA DE ARCHIVOS CENTRALIZADO. LA
COMPLEJIDAD DE LA DISTRIBUCIÓN ES OCULTADA A LOS USUARIOS
FINALES.
   CARACTERÍSTICAS SISTEMAS DE ARCHIVOS
LOS SISTEMAS DE ARCHIVOS DISTRIBUIDOS PUEDEN IMPLEMENTAR POLÍTICAS DE
                         DISTRIBUDIOS
REDUNDANCIA PARA GARANTIZAR QUE LOS DATOS ESTÉN DISPONIBLES CUANDO UNO
O VARIOS NODOS FALLAN. ESTO AUMENTA LA FIABILIDAD Y LA DISPONIBILIDAD DEL
SISTEMA.

SON DISEÑADOS PARA SER ESCALABLES HORIZONTALMENTE, LO QUE SIGNIFICA QUE
SE PUEDEN AGREGAR MÁS NODOS A MEDIDA QUE SE NECESITE MÁS CAPACIDAD DE
ALMACENAMIENTO O RENDIMIENTO.

LOS SISTEMAS DE ARCHIVOS DISTRIBUIDOS TIENEN QUE GESTIONAR METADATOS,
QUE SON LA INFORMACIÓN SOBRE LOS DATOS, COMO NOMBRES DE ARCHIVOS,
UBICACIONES Y PERMISOS.
              ¿QUÉ ES HADOOP?
HADOOP ES UN FRAMEWORK DE CÓDIGO ABIERTO PARA
ALMACENAR DATOS Y EJECUTAR APLICACIONES EN
CLÚSTERES DE HARDWARE COMERCIAL. PROPORCIONA
ALMACENAMIENTO MASIVO PARA CUALQUIER TIPO DE
DATOS, TIENE UN ENORME PODER DE PROCESAMIENTO Y
LA CAPACIDAD DE PROCESAR TAREAS O TRABAJOS
CONCURRENTES VIRTUALMENTE ILIMITADOS.
               ¿QUÉ ES HADOOP?
DOUG CUTTING, UNO DE LOS CREADORES DE HADOOP,
ELIGE EL NOMBRE POR UN ELEFANTE DE JUGUETE QUE
PERTENECÍA A SU HIJO, Y HABÍA SIDO BAUTIZADO CON
ESTE NOMBRE. LA ROBUSTEZ QUE SIMBOLIZA EL ELEFANTE
TAMBIÉN COLABORÓ PARA ELEGIR ESTE ANIMAL COMO
LOGO PARA ESTA TECNOLOGÍA, YA QUE ESA ES LA IMAGEN
QUE SE QUIERE TRANSMITIR COMO PRINCIPAL CUALIDAD.
              HISTORIA HADOOP

EN 2003 DOUG CUTTING, CREADOR DE LUCENE, JUNTO A
MIKE CAFARELLA, ESTABAN TRABAJANDO EN NOUCH, UN
MOTOR DE BÚSQUEDA WEB OPEN SOURCE. POR EL
TAMAÑO DE LO QUE ERA LA WEB YA EN 2003 SE
NECESITABAN DESARROLLAR UN PRODUCTO QUE FUERA
ESCALABLE.
           HISTORIA HADOOP



¿SE ENFRENTABAN A UN PROBLEMA DE BIG
DATA?
            HISTORIA HADOOP


SI, NO SE PUEDE RECORRER TODA LA WEB,
INDEXARLA Y GUARDAR TODA LA METADATA DE
CADA SITIO CON MÉTODOS CONVENCIONALES.
¿CUÁL FUE LA
              HISTORIA  HADOOP
            SOLUCIÓN? A PARTIR DE CIERTOS PAPERS
PUBLICADOS POR GOOGLE DE COMO HACÍAN PARA
TRABAJAR INTERNAMENTE, DOUG Y MIKE DECIDIERON
REPLICAR ESE TRABAJO PARA HACERLO OPEN SOURCE Y
DEJARLO DISPONIBLE PARA TODA LA COMUNIDAD. EN
PARTICULAR EL SISTEMA DE ARCHIVOS TFS Y EL
FRAMEWORK MAP REDUCE. PARA 2006 YAHOO INVITRIÓ EN
EL PROYECTO, PASANDO CUTTING A FORMAR PARTE DE LA
EMPRESA.
               HISTORIA HADOOP
CON LAS PARTES QUE ERAN DISTRIBUIDAS DE NUTCH,
COMENZARON UN NUEVO PROYECTO BAJO EL NOMBRE DE
HADOOP. EL PROYECTO TUVO GRAN ÉXITO Y FUE
UTILIZADO POR VARIAS ORGANIZACIONES, SOBRE TODO
COMPAÑÍAS DE INTERNET. A PARTIR DE ENTONCES OTRAS
HERRAMIENTAS SE FUERON DESARROLLANDO Y SUMANDO
AL ECOSISTEMA HADOOP, PASANDO ESTE A SER LA MEJOR
PLATAFROMA PARA PROCESAR GRANDES VOLÚMENES DE
DATOS.
   COMPONENTES PRINCIPALES DE HADOOP

HADOOP TIENE DOS COMPONENTES PRINCIPALES.
 • UN LUGAR DONDE ALMACENA LOS DATOS. PARA ESO
   UTILIZA HDFS (HADOOP DISTRIBUTED FILE SYSTEM)
 • UNA FORMA DE       PROCESAR    LOS    DATOS,  CON
   MAPREDUCE.
                       HDFS
ES UN SISTEMA DE ARCHIVOS PORTÁTIL, ESCALABLE Y
DISTRIBUIDO BASADO EN JAVA, DISEÑADO PARA ABARCAR
GRANDES CLÚSTERES DE SERVIDORES. SU DISEÑO SE BASA EN
TFS, EL SISTEMA DE ARCHIVOS DE GOOGLE. HDFS PUEDE
ALMACENAR UNA GRAN CANTIDAD DE DATOS Y PROPORCIONAR
ACCESO A VARIOS CLIENTES DISTRIBUIDOS A TRAVÉS DE UNA
RED. HDFS SOBRESALE EN SU CAPACIDAD PARA ALMACENAR
ARCHIVOS GRANDES DE MANERA CONFIABLE Y ESCALABLE.
         HDFS (MODELO MASTER SLAVE)
HDFS UTILIZA EL ESQUEMA CONOCIDO COMO MASTER SLAVE
PARA COORDINAR LOS NODOS DEL SISTEMA. ES UN MODELO DE
COMUNICACIÓN     O   CONTROL  ASIMÉTRICO   DONDE   UN
DISPOSITIVO O PROCESO CONTROLA UNO O MÁS DISPOSITIVOS
O PROCESOS Y SIRVE COMO SU CENTRO DE COMUNICACIÓN. EN
ALGUNOS SISTEMAS, SE SELECCIONA UN MAESTRO DE UN
GRUPO    DE   DISPOSITIVOS ELEGIBLES,  Y  LOS   OTROS
DISPOSITIVOS ACTÚAN EN EL PAPEL DE ESCLAVOS SIGUIENDO
SUS ÓRDENES.
HDFS (MODELO MASTER SLAVE)
                       HDFS
HDFS ESTÁ DISEÑADO PARA ALMACENAR MUCHA INFORMACIÓN.
GENERALMENTE PETABYTES, GIGABYTES Y TERABYTES. ESTO
SE LOGRA MEDIANTE EL USO DE UN SISTEMA DE ARCHIVOS
ESTRUCTURADO EN BLOQUES. LOS ARCHIVOS INDIVIDUALES SE
DIVIDEN EN BLOQUES DE TAMAÑO FIJO QUE SE ALMACENAN EN
MÁQUINAS EN TODO EL CLÚSTER. LOS ARCHIVOS COMPUESTOS
DE VARIOS BLOQUES GENERALMENTE NO TIENEN TODOS SUS
BLOQUES ALMACENADOS EN UNA SOLA MÁQUINA.
                        HDFS
HDFS GARANTIZA LA INTEGRIDAD DE LOS DATOS MEDIANTE LA
REPLICACIÓN DE BLOQUES Y LA DISTRIBUCIÓN DE LAS
RÉPLICAS EN TODO EL CLÚSTER. EL FACTOR DE REPLICACIÓN
PREDETERMINADO ES TRES, LO QUE SIGNIFICA QUE CADA
BLOQUE EXISTE TRES VECES EN EL CLÚSTER. LA REPLICACIÓN
PERMITE LA DISPONIBILIDAD DE DATOS INCLUSO CUANDO
FALLAN LAS MÁQUINAS.
                                   REDUNDANCIA
Tengo un   Lo particiono   Distribuyo los
 archivo    en bloques      bloques en
                           cada servidor
                                   REDUNDANCIA
                                                             Hago copia de
Tengo un   Lo particiono   Distribuyo los
                                                               cada bloque
 archivo    en bloques      bloques en
                                                             según un factor
                           cada servidor
                                                              de replicación



                                                                         Este no procesa
                                                                         datos. Es el que
                                                                          orquesta a los
                                                                             demás




                                            Además hay uno
                                              de resplado
                       HDFS

EL DISEÑO ARQUITECTÓNICO DE HDFS SE COMPONE DE DOS
PARTES: EL NAMENODE QUE CONTIENE LOS METADATOS PARA
EL SISTEMA DE ARCHIVOS, Y UNO O MÁS DATANODES QUE
ALMACENAN LOS BLOQUES QUE COMPONEN LOS ARCHIVOS.
ESTOS SON LOS NOMBRES QUE SE UTILIZAN EN HADOOP PARA
EL NODO MASTER Y LOS NODOS SLAVE.
                      HDFS

LOS NAMENODE Y DATANODE PUEDEN EJECUTARSE EN UNA
SOLA MÁQUINA, PERO LOS CLÚSTERES DE HDFS COMÚNMENTE
CONSTAN DE UN SERVIDOR DEDICADO QUE EJECUTA EL
NAMENODE Y VARIAS MÁQUINAS QUE EJECUTAN LOS
DATANODES.
                        HDFS

NAMENODE ES LA MÁQUINA MÁS IMPORTANTE EN HDFS.
ALMACENA METADATOS PARA TODO EL SISTEMA DE ARCHIVOS:
NOMBRES DE ARCHIVOS, PERMISOS Y LA UBICACIÓN DE CADA
BLOQUE DE CADA ARCHIVO. PARA PERMITIR UN ACCESO RÁPIDO
A ESTA INFORMACIÓN, EL NAMENODE ALMACENA TODA LA
ESTRUCTURA DE METADATOS EN MEMORIA Y NO EN DISCO.
                        HDFS

NAMENODE TAMBIÉN REALIZA UN SEGUIMIENTO DEL FACTOR DE
REPLICACIÓN DE LOS BLOQUES, LO QUE GARANTIZA QUE LAS
FALLAS DE LA MÁQUINA NO PROVOQUEN LA PÉRDIDA DE DATOS.
SE ASEGURA QUE LA INFORMACIÓN ESTÉ REPLICADA LA
CANTIDAD DE VECES QUE FUE INDICADO.
                              HDFS
¿QUÉ PASA SI EL NAMENODE FALLA?

SE PUEDE USAR UN NAMENODE SECUNDARIO PARA GENERAR INSTANTÁNEAS
(SNAPSHOTS DE RESPALDO) DE LAS ESTRUCTURAS DE MEMORIA DEL
NAMENODE PRINCIPAL, LO QUE REDUCE EL RIESGO DE PÉRDIDA DE DATOS SI EL
NAMENODE DEJA DE FUNCIONAR.

VAMOS A TENER ENTONCES DOS NAMENODE:
 • ACTIVE NAMENODE
 • STANDBY NAMENODE
                      HDFS
LAS MÁQUINAS QUE ALMACENAN LOS BLOQUES DENTRO DE
HDFS SE DENOMINAN DATANODES. LOS DATANODES SUELEN
SER     SERVIDORES  CON   GRANDE    CAPACIDAD   DE
ALMACENAMIENTO. A DIFERENCIA DE NAMENODE, HDFS
CONTINUARÁ FUNCIONANDO NORMALMENTE SI FALLA UN
DATANODE. CUANDO UN DATANODE FALLA, EL NAMENODE
REPLICARÁ LOS BLOQUES PERDIDOS PARA GARANTIZAR QUE
CADA BLOQUE CUMPLA CON EL FACTOR DE REPLICACIÓN
MÍNIMO.
                   MAPREDUCE

¿CÓMO FUNCIONA MAP REDUCE?

EN VEZ QUE LOS DATOS VENGAN AL CÓDIGO. EL CÓDIGO VA A
LOS DATOS.
                   MAPREDUCE

¿CÓMO FUNCIONA MAP REDUCE?

EN VEZ QUE LOS DATOS VENGAN AL CÓDIGO. EL CÓDIGO VA A
LOS DATOS.
         ¿CÓMO FUNCIONA MAPREDUCE?
EJEMPLO.
TENGO EL SIGUIENTE DATA SET:




QUIERO SABER LA SUMA DE LOS VALORES PARA CADA CLAVE
        ¿CÓMO FUNCIONA MAPREDUCE?
MAP




MAPEO CADA CLAVE Y VOY RECOLECTANDO LOS VALORES QUE
CORRESPONDEN A CADA CLAVE.
        ¿CÓMO FUNCIONA MAPREDUCE?
MAP




MAPEO CADA CLAVE Y VOY RECOLECTANDO LOS VALORES QUE
CORRESPONDEN A CADA CLAVE.
        ¿CÓMO FUNCIONA MAPREDUCE?
MAP




MAPEO CADA CLAVE Y VOY RECOLECTANDO LOS VALORES QUE
CORRESPONDEN A CADA CLAVE.
         ¿CÓMO FUNCIONA MAPREDUCE?
REDUCE




LUEGO REDUZCO LA INFORMACIÓN. EN ESTE CASO MEDIANTE
LA SUMA DE LOS VALORES.
        ¿CÓMO FUNCIONA MAPREDUCE?

OTRO EJEMPLO ES LA FUNCIÓN WORD COUNT QUE ME INDICA
LAS PALABRAS QUE HAY EN UNO O VARIOS DOCUMENTOS, Y LA
CANTIDAD DE OCURRENCIAS DE CADA UNA.
ESTE ES EL MISMO PRINCIPIO QUE UTILIZA HADOOP PARA
CONSULTAR LA INFORMACIÓN DISTRIBUIDA A LO LARGO DE
VARIOS SERVIDORES.
        ¿CÓMO FUNCIONA MAPREDUCE?
MAP REDUCE ES UN ALGORITMO QUE PERMITE PROCESAR
GRANDES VOLÚMENES DE DATOS DIVIDIENDO EL TRABAJO EN
PEQUEÑAS TAREAS INDEPENDIENTES (TASKS) Y EJECUTANDO
(CON EXECUTORS) ESAS TAREAS EN PARALELO A TRAVÉS DE UN
CLÚSTER.   EL   ALGORITMO   ESTÁ   INSPIRADO   EN   LA
PROGRAMACIÓN FUNCIONAL TOMANDO LAS FUNCIONES DE
MAPEO Y REDUCCIÓN, QUE SON COMÚNMENTE UTILIZADAS PARA
PROCESAR LISTAS DE DATOS.
              ¿CÓMO FUNCIONA MAPREDUCE?

     Función Mapper              Shuffle and Sort             Función Reduce
MAPEA DISTINTOS PARES DE   EL PROCESO DE MOVER LOS
                                                         LA FUNCIÓN REDUCE ITERA
CLAVES-VALOR.    PROCESA   OUTPUTS DEL MAPPER A LOS
                                                         POR LOS DATOS PARA CADA
SECUENCIALMENTE    ESTOS   REDUCTORES      SE   LLAMA
                                                         CLAVE,         APLICANDO
PARES DE CLAVE VALOR,      SHUFFLE. SE ENCARGA DE
                                                         FUNCIONES DE AGREGACIÓN
PRODUCIENDO NINGÚNO O      QUE TODOS LOS VALORES DE
                                                         Y DEVOLVIENDO LUEGO LOS
VARIOS          OUTPUTS.   LA MISMA CLAVE, VAYAN AL
                                                         OUTPUT.
DEPENDIENDO SI HAY O NO    MISMO    REDUCTOR.     POR
DATOS PARA LAS CLAVES      DEFAULT SE UTILIZA HASHING.
BUSCADAS.                  LUEGO SE ORDENAN LOS
                           DATOS.
        ¿CÓMO FUNCIONA MAPREDUCE?
MAP REDUCE ES UN ALGORITMO QUE PERMITE PROCESAR
GRANDES VOLÚMENES DE DATOS DIVIDIENDO EL TRABAJO EN
PEQUEÑAS TAREAS INDEPENDIENTES (TASKS) Y EJECUTANDO
(CON EXECUTORS) ESAS TAREAS EN PARALELO A TRAVÉS DE UN
CLÚSTER.   EL   ALGORITMO   ESTÁ   INSPIRADO   EN   LA
PROGRAMACIÓN FUNCIONAL TOMANDO LAS FUNCIONES DE
MAPEO Y REDUCCIÓN, QUE SON COMÚNMENTE UTILIZADAS PARA
PROCESAR LISTAS DE DATOS.
             SISTEMAS DE BUCKETS
UN BUCKET ES UN CONTENEDOR VIRTUAL, NORMALMENTE
ALMACENADO EN LA NUBE, AUNQUE TAMBIÉN SE PUEDE TENER
ON PREMISE, DONDE LOS USUARIOS ACCEDEN A TRAVÉS DE
APIS O URLS ESPECIFICAS. CADA BUCKET TIENE UN
IDENTIFICADOR ÚNICO DENTRO DEL PROVEEDOR CLOUD.
ADMITE DENTRO DE SI MÍSMO UNA JERARQUÍA DE ARCHIVOS
PERO EN EL PARADIGMA DE BUCKETS LOS TRATAMOS COMO
OBJETOS. CADA OBJETO TIENE SU PROPIA CLAVE.
              SISTEMAS DE BUCKETS
LOS BUCKETS SON UTILIZADOS COMO REPOSITORIOS DE
GRANDES VOLUMENES DE DATOS EN ORGANZIACIONES, PERO
TAMBIÉN PARA CASOS DE USO QUE NO SON DE BIG DATA, COMO
SUBIR DATOS DE UNA WEB QUE SE PRECARGAN EN EL BUCKET.
PODEMOS USAR BUCKETS PARA EL ALMACENAMIENTO DE LOS
DATOS DE NUESTRO DATALAKE, YA QUE AL SER ALTAMENTE
ESCALABLES, SIRVEN PARA EL USO EN PROBLEMAS DE BIG
DATA.
03 - Herramientas
      de ETL
    COMPONENTES DEL DATALAKE
                                               Visualización y
                                                  Consumo
Fuentes
de Datos   DW
                              Almacenamiento




                              Procesamiento
                ETL


                                Modelado y
                                Consultas

                      Usted
                      está
                      aquí
             ¿QUÉ SIGNIFICA
ETL   SIGNIFICA  ETL?
                 "EXTRACT,  TRANSFORM,    LOAD"  (EXTRACCIÓN,
TRANSFORMACIÓN Y CARGA, EN ESPAÑOL). ES UN PROCESO UTILIZADO
EN EL ÁMBITO DE LA GESTIÓN Y ANÁLISIS DE DATOS PARA MOVER Y
TRANSFORMAR DATOS DESDE MÚLTIPLES FUENTES HACIA UN SISTEMA
DE ALMACENAMIENTO CENTRALIZADO, COMO PUEDE SER UNA BASE DE
DATOS, UN DATA WAREHOUSE O UN DATALAKE. EL PROCESO ETL CONSTA
DE TRES FASES.
             ¿QUÉ SIGNIFICA
                 ETL?   EXTRACCIÓN:

EN ESTA ETAPA, LOS DATOS SE EXTRAEN DE DIVERSAS FUENTES, QUE
PUEDEN SER BASES DE DATOS, ARCHIVOS PLANOS, SERVICIOS WEB,
APIS U OTRAS FUENTES DE DATOS. LOS DATOS EXTRAÍDOS SUELEN SER
DATOS BRUTOS O SEMIESTRUCTURADOS EN SU FORMA ORIGINAL.
TAMBIÉN PUEDEN SER DATOS NO ESTRUCTURADOS COMO AUDIOS,
FOTOS, VIDEOS, ENTRE OTROS.
             ¿QUÉ SIGNIFICA
                 ETL? TRANSFORMACIÓN:

DESPUÉS DE LA EXTRACCIÓN, LOS DATOS SE SOMETEN A PROCESOS DE
TRANSFORMACIÓN. DURANTE ESTA ETAPA, LOS DATOS SE LIMPIAN, SE
AJUSTAN   Y  SE   REESTRUCTURAN    SEGÚN   LAS    NECESIDADES
ESPECÍFICAS DEL ANÁLISIS O DEL SISTEMA DE DESTINO. LAS
TRANSFORMACIONES PUEDEN INCLUIR FILTRADO, LIMPIEZA DE DATOS
INCORRECTOS    O  INCOMPLETOS,   CONVERSIÓN    DE   FORMATOS,
CÁLCULOS Y AGREGACIONES.
             ¿QUÉ SIGNIFICA
                 ETL?     CARGA:

UNA VEZ QUE LOS DATOS HAN SIDO EXTRAÍDOS Y TRANSFORMADOS, SE
CARGAN EN EL SISTEMA DE DESTINO, QUE SUELE SER UNA BASE DE
DATOS, UN DATA WAREHOUSE O UN DATALAKE.
DEPENDIENDO DE LA FRECUENCIA DEL PROCESO ETL, LOS DATOS
PUEDEN CARGARSE EN LOTES PERIÓDICOS (PROCESAMIENTO EN
BATCHES) O EN TIEMPO REAL (STREAMING).
¿SIEMPRE ES EN ESE ORDEN?
¿SIEMPRE ES EN ESE ORDEN?
     ¿QUÉ TIPOS DE TRANSFORMACIÓN SON
                       POSIBLES?
FILTRADO DE DATOS: SE ELIMINAN FILAS DE DATOS QUE NO CUMPLEN
CON CIERTOS CRITERIOS O CONDICIONES ESPECÍFICAS. POR EJEMPLO,
PUEDES FILTRAR REGISTROS CON FECHAS FUTURAS O VALORES NULOS.

ELIMINACIÓN DE DUPLICADOS: SE IDENTIFICAN Y ELIMINAN REGISTROS
DUPLICADOS EN FUNCIÓN DE UNA O VARIAS COLUMNAS CLAVE. ESTO
GARANTIZA LA INTEGRIDAD DE LOS DATOS Y EVITA PROBLEMAS EN EL
ANÁLISIS.
    ¿QUÉ TIPOS DE TRANSFORMACIÓN SON
NORMALIZACIÓN DE DATOS:POSIBLES?
                        SE AJUSTAN LOS DATOS A UN FORMATO
ESTÁNDAR.  POR   EJEMPLO, PUEDES CONVERTIR TODAS  LAS
DIRECCIONES A MAYÚSCULAS O ESTANDARIZAR FECHAS EN UN
FORMATO ESPECÍFICO.

CONCATENACIÓN DE COLUMNAS: SE COMBINAN DOS O MÁS COLUMNAS
EN UNA SOLA. ESTO PUEDE SER ÚTIL PARA CREAR CLAVES COMPUESTAS
O GENERAR VALORES CALCULADOS.
     ¿QUÉ TIPOS DE TRANSFORMACIÓN SON
DIVISIÓN DE COLUMNAS: POSIBLES?
                      SE DIVIDE UNA COLUMNA EN MÚLTIPLES
COLUMNAS. ESTO ES ÚTIL CUANDO SE TIENE UNA COLUMNA QUE
CONTIENE INFORMACIÓN COMBINADA QUE DEBE DESGLOSARSE.

CONVERSIÓN DE TIPOS DE DATOS: SE CAMBIAN LOS TIPOS DE DATOS DE
UNA COLUMNA PARA QUE SEAN COMPATIBLES CON EL DESTINO. POR
EJEMPLO, CONVERTIR UNA CADENA DE TEXTO QUE REPRESENTA UNA
FECHA EN UN TIPO DE DATO DE FECHA.
    ¿QUÉ TIPOS DE TRANSFORMACIÓN SON
                      POSIBLES?
ENRIQUECIMIENTO DE DATOS: SE AGREGAN DATOS ADICIONALES    A
PARTIR DE FUENTES EXTERNAS PARA ENRIQUECER LA INFORMACIÓN
EXISTENTE. POR EJEMPLO, AGREGAR DATOS DEMOGRÁFICOS A PARTIR
DE CÓDIGOS POSTALES.

CÁLCULOS   Y   AGREGACIONES:  SE    REALIZAN  OPERACIONES
MATEMÁTICAS, COMO SUMAS, PROMEDIOS O CONTEOS, EN COLUMNAS
NUMÉRICAS. ESTO ES ESENCIAL PARA GENERAR MÉTRICAS Y
RESÚMENES.
    ¿QUÉ TIPOS DE TRANSFORMACIÓN SON
MAPEO DE VALORES: SE REEMPLAZAN VALORES EXISTENTES POR
                    POSIBLES?
OTROS SEGÚN UN MAPEO PREDEFINIDO. POR EJEMPLO, CONVERTIR
CÓDIGOS DE PRODUCTO EN NOMBRES DE PRODUCTOS.

DERIVACIÓN DE NUEVAS COLUMNAS: SE CREAN NUEVAS COLUMNAS
CALCULADAS A PARTIR DE DATOS EXISTENTES. POR EJEMPLO,
CALCULAR LA EDAD A PARTIR DE LA FECHA DE NACIMIENTO.

LIMPIEZA DE DATOS INCONSISTENTES: SE CORRIGEN DATOS CON
ERRORES    TIPOGRÁFICOS, VALORES   ATÍPICOS O  FORMATOS
INCORRECTOS.
 ¿POR QUÉ HACEMOS PROCESOS DE
CENTRALIZACIÓN Y ALMACENAMIENTO EFICIENTE: LOS DATOS ESTÁN
                         ETL?JUNTARLOS EN UN SISTEMA DE
PRESENTES EN DIVERSAS FUENTES,
ALMACENAMIENTO CENTRALIZADO, COMO UN DATAWAREHOUSE O UN
DATA LAKE, FACILITA EL ACCESO Y LA ADMINISTRACIÓN DE LA
INFORMACIÓN EN UN SOLO LUGAR PARA SER ALMACENADA Y
RESPALDADA.
ANÁLISIS Y TOMA DE DECISIONES: MOVERLOS A UN LUGAR
CENTRALIZADO TAMBIÉN PERMITE REALIZAR ANÁLISIS MÁS EFECTIVOS Y
TOMAR DECISIONES INFORMADAS BASADAS EN UNA VISIÓN COMPLETA
DE LA INFORMACIÓN.
 ¿POR QUÉ HACEMOS PROCESOS DE
                        ETL?
INTEGRACIÓN DE SISTEMAS: ALGUNAS ORGANIZACIONES TIENEN
SISTEMAS AISLADOS QUE NO SE COMUNICAN ENTRE SÍ. MOVER DATOS
ENTRE ESTOS SISTEMAS PERMITE UNA MEJOR INTEGRACIÓN Y
AUTOMATIZACIÓN DE PROCESOS.
CUMPLIMIENTO NORMATIVO: EN ALGUNOS CASOS, ES NECESARIO
MOVER DATOS PARA CUMPLIR CON REGULACIONES Y LEYES QUE
REQUIEREN LA RETENCIÓN O ELIMINACIÓN ADECUADA DE INFORMACIÓN.
¿POR QUÉ HACEMOS PROCESOS DE
            ETL?
INTEGRACIÓN DE DATOS: TAMBIÉN CONOCIDA COMO DATA INTEGRATION
EN INGLÉS, SE REFIERE AL PROCESO DE COMBINAR DATOS DE
DIVERSAS FUENTES Y FORMATOS EN UNA SOLA VISTA COHERENTE Y
UNIFICADA. LOS DATOS DE VARIOS SISTEMAS SE TOMAN PARA HACER UN
MODELO DE DATOS NUEVO.
 ¿POR QUÉ NO HACEMOS ANALÍTICA DIRECTO CONTRA LAS
                FUENTES DE DATOS?
LAS FUENTES DE DATOS, SOBRE TODO LAS ESTRUCTURADAS,
NORMALMENTE     ESTÁN   DISEÑADAS    PARA   ESCRIBIR   DATOS
RÁPIDAMENTE, PERO NO PARA LEER UNA GRAN CANTIDAD DE DATOS DE
FORMA RÁPIDA. ESTÁS BASES DE DATOS SON TRANSACCIONALES Y NO
ANALÍTICAS. HACER CONSULTAS EN ESTOS SISTEMAS ES LENTO Y
ENTORPECE LA ESCRITURA. LO IMPORTANTE DE ESTA BASE DE DATOS
ES PERSISTIR LA INFORMACIÓN DE LA APLICACIÓN O MICROSERVICIO
QUE ESTÁ GENERANDO LOS DATOS.
 ¿POR QUÉ NO HACEMOS ANALÍTICA DIRECTO CONTRA LAS
                FUENTES DE DATOS?

LOS MODELOS ANALÍTICOS COMO LOS QUE ARMAMOS EN UN
DATAWAREHOUSE O DATALAKE SON MODELOS QUE ESTÁN OPTIMIZADOS
PARA UNA LECTURA RÁPIDA, SACRIFICANDO LA ESCRITURA DE DATOS.
POR ESTE MOTIVO ES RECOMENDABLE HACER LOS PROCESOS DE ETL
QUE ESCRIBEN GRANDES VOLUMENES DE DATOS, EN HORAS DE BAJA
DEMANDA, DONDE NO SE ESTÉN REALIZANDO CONSULTAS.
                  OLTP VS OLAP

NOS ENFRENTAMOS A DOS MODELOS DIFERENTES A LA HORA DE
GESTIONAR SISTEMAS DE DATOS.
OLAP (PROCESAMIENTO ANALÍTICO EN LÍNEA) Y OLTP (PROCESAMIENTO
DE TRANSACCIONES EN LÍNEA) SON DOS ENFOQUES CLAVE EN LA
GESTIÓN DE BASES DE DATOS QUE SE UTILIZAN PARA DIFERENTES
TIPOS DE APLICACIONES Y REQUERIMIENTOS.
                          OLTP
OLTP SE ENFOCA EN LA GESTIÓN DE TRANSACCIONES Y OPERACIONES
DIARIAS DE UNA ORGANIZACIÓN. SU OBJETIVO PRINCIPAL ES ADMITIR
PROCESOS EMPRESARIALES COMO VENTAS, COMPRAS Y RESERVAS EN
TIEMPO REAL.

LAS OPERACIONES OLTP SON PRINCIPALMENTE DE ESCRITURA Y
LECTURA. LOS USUARIOS REALIZAN TRANSACCIONES QUE ACTUALIZAN
Y CONSULTAN DATOS CONSTANTEMENTE. LA INTEGRIDAD DE LOS DATOS
Y LA CONSISTENCIA SON CRÍTICAS.
                          OLTP
OLTP UTILIZA UN MODELO DE DATOS RELACIONAL, DONDE LOS DATOS SE
ORGANIZAN EN TABLAS NORMALIZADAS PARA GARANTIZAR LA
INTEGRIDAD Y LA CONSISTENCIA DE LOS DATOS.

LOS SISTEMAS OLTP ESTÁN OPTIMIZADOS PARA LA VELOCIDAD DE
RESPUESTA Y LA CONCURRENCIA. SE CENTRAN EN MANTENER LA
INTEGRIDAD DE LOS DATOS Y ADMINISTRAR TRANSACCIONES
CONCURRENTES.
OLAP SE CENTRA EN EL
                          OLAP
                       ANÁLISIS DE DATOS
                                      Y LA GENERACIÓN DE
INFORMES. SU OBJETIVO PRINCIPAL ES PROPORCIONAR A LOS
USUARIOS LA CAPACIDAD DE CONSULTAR GRANDES CONJUNTOS DE
DATOS  PARA   OBTENER  INFORMACIÓN  Y  TOMAR   DECISIONES
ESTRATÉGICAS.

LAS OPERACIONES OLAP SON PRINCIPALMENTE DE LECTURA. LOS
USUARIOS REALIZAN CONSULTAS COMPLEJAS PARA RESUMIR, AGRUPAR,
FILTRAR Y ANALIZAR DATOS. LAS ACTUALIZACIONES DE DATOS EN
SISTEMAS OLAP SUELEN SER MÍNIMAS Y PLANIFICADAS, COMO LA CARGA
DE DATOS PERIÓDICA.
                          OLAP
OLAP UTILIZA UN MODELO DE DATOS MULTIDIMENSIONAL, DONDE LOS
DATOS SE ORGANIZAN EN CUBOS O ESTRUCTURAS SIMILARES QUE
REPRESENTAN DIMENSIONES Y MEDIDAS. ESTO FACILITA LAS
CONSULTAS Y EL ANÁLISIS DE DATOS.

LOS SISTEMAS OLAP ESTÁN OPTIMIZADOS PARA CONSULTAS Y ANÁLISIS
DE ALTO RENDIMIENTO. SUELEN UTILIZAR AGREGACIONES Y PRE-
CÁLCULOS PARA ACELERAR LAS CONSULTAS COMPLEJAS.
          HERRAMIENTAS DE
               ETL
EXISTE UNA VARIEDAD MUY GRANDE DE HERRAMIENTAS PARA REALIZAR
TAREAS DE ETL. ALGUNAS SON OPEN SOURCE, Y OTRAS SON
ÚNICAMENTE DE PROVEEDORES CLOUD. DENTRO DE LAS OPEN
SOURCE, MUCHAS NACIERON CON EL ENFOQUE EN ANALYTICS PARA
DATA WAREHOUSES TRADICIONALES Y MÁS TARDE SE ADAPTARON AL
MUNDO DE BIG DATA. MUCHAS DE ELLAS ESTÁN TAMBIÉN DENTRO DE
SUITES DE PRODUCTOS QUE TIENEN MÁS FUNCIONALIDADES ADEMÁS
DE SER UNA HERRAMIENTA DE ETL.
         HERRAMIENTAS DE
              ETL
PODEMOS ENCONTRAR COMO EJEMPLO:
 • PENTAHO
 • TALEND
 • GLUE
 • CLOUD DATA FUSION
 • AZURE DATA FACTORY
 • NIFI
                    PENTAHO
PENTAHO ES UNA PLATAFORMA DE INTELIGENCIA EMPRESARIAL DE
CÓDIGO ABIERTO QUE OFRECE UNA SUITE COMPLETA DE HERRAMIENTAS
PARA AYUDAR EN LA GESTIÓN, ANÁLISIS Y VISUALIZACIÓN DE DATOS.
FUE DESARROLLADA ORIGINALMENTE POR PENTAHO CORPORATION Y
LUEGO ADQUIRIDA POR HITACHI DATA SYSTEMS EN 2015. LA SUITE
PENTAHO INCLUYE VARIAS HERRAMIENTAS QUE ABARCAN DIFERENTES
ASPECTOS DEL PROCESAMIENTO DE DATOS Y LA INTELIGENCIA
EMPRESARIAL.
                    PENTAHO
PENTAHO DATA INTEGRATION (PDI): TAMBIÉN CONOCIDO COMO KETTLE,
ES UNA HERRAMIENTA ETL QUE PERMITE EXTRAER, TRANSFORMAR Y
CARGAR DATOS DESDE DIVERSAS FUENTES HACIA DESTINOS COMO
BASES DE DATOS, ALMACENES DE DATOS Y OTROS SISTEMAS.
PENTAHO BIG DATA INTEGRATION: UNA EXTENSIÓN DE PDI DISEÑADA
PARA TRABAJAR CON GRANDES VOLÚMENES DE DATOS Y ENTORNOS DE
BIG DATA.
                     TALEND
TALEND ES UNA PLATAFORMA DE INTEGRACIÓN DE DATOS QUE SE
UTILIZA  COMÚNMENTE   PARA  REALIZAR   PROCESOS   DE  ETL.
PROPORCIONA UNA SERIE DE HERRAMIENTAS Y COMPONENTES QUE
PERMITEN A LOS USUARIOS EXTRAER DATOS DE DIVERSAS FUENTES,
TRANSFORMARLOS DE ACUERDO A SUS NECESIDADES Y CARGARLOS EN
DESTINOS ESPECÍFICOS.
                     TALEND
TALEND DATA INTEGRATION: ESTA ES LA PARTE CENTRAL DE LA
PLATAFORMA Y SE CENTRA EN EL PROCESO DE ETL. OFRECE UNA
INTERFAZ GRÁFICA PARA DISEÑAR FLUJOS DE DATOS QUE INVOLUCRAN
EXTRACCIÓN, TRANSFORMACIÓN Y CARGA DE DATOS. LOS USUARIOS
PUEDEN DEFINIR TRANSFORMACIONES COMPLEJAS Y CONFIGURAR
CONEXIONES A DIVERSAS FUENTES DE DATOS Y DESTINOS.
                      TALEND
TALEND PROPORCIONA CONECTORES PARA UNA AMPLIA VARIEDAD DE
SISTEMAS DE BIG DATA, INCLUIDOS HADOOP, SPARK Y MÁS. ESTO
PERMITE A LOS USUARIOS CONECTARSE A PLATAFORMAS PARA BIG DATA
Y TRABAJAR CON SUS DATOS DE MANERA DIRECTA.
TAMBIÉN ADQUIRIÓ STITCH, PLATAFORMA DE INTEGRACIÓN DE DATOS
QUE SE CENTRA EN LA CARGA DE DATOS DESDE DIVERSAS FUENTES
HACIA DESTINOS DE DATOS, COMO ALMACENES DE DATOS, DATA LAKES
Y BASES DE DATOS EN LA NUBE.
                       GLUE
AWS GLUE ES UN SERVICIO DE EXTRACCIÓN, TRANSFORMACIÓN Y
CARGA ADMINISTRADO PROPORCIONADO POR AMAZON WEB SERVICES.
ESTÁ DISEÑADO ESPECÍFICAMENTE PARA TRABAJAR CON GRANDES
VOLÚMENES DE DATOS Y ENTORNOS DE BIG DATA EN LA NUBE. GLUE
AUTOMATIZA GRAN PARTE DEL PROCESO ETL AL PROPORCIONAR
HERRAMIENTAS Y RECURSOS PARA DESCUBRIR, TRANSFORMAR Y
MOVER DATOS DESDE DIVERSAS FUENTES HACIA DESTINOS EN AWS,
COMO PUEDE SER UN HDFS QUE TENGAMOS DESPLEGADO, O UN S3.
         CLOUD DATA FUSION
CLOUD DATA FUSION ES UN SERVICIO DE GOOGLE CLOUD PLATFORM
QUE OFRECE CAPACIDADES DE INTEGRACIÓN Y TRANSFORMACIÓN DE
DATOS EN LA NUBE. SE BASA EN LA PLATAFORMA OPEN SOURCE APACHE
NIFI Y PROPORCIONA UNA INTERFAZ VISUAL PARA DISEÑAR, EJECUTAR Y
ADMINISTRAR FLUJOS DE DATOS EN ENTORNOS DE BIG DATA. EL
OBJETIVO PRINCIPAL DE CLOUD DATA FUSION ES FACILITAR EL PROCESO
DE ETL Y LA PREPARACIÓN DE DATOS EN LA NUBE, PERMITIENDO A LOS
USUARIOS DISEÑAR FLUJOS DE TRABAJO A TRAVÉS DE UNA INTERFAZ
GRÁFICA SIN NECESIDAD DE ESCRIBIR CÓDIGO.
                 AZURE DATA
                  FACTORY
AZURE DATA FACTORY ES UN SERVICIO DE MICROSOFT AZURE QUE
PERMITE LA CREACIÓN, ORQUESTACIÓN Y AUTOMATIZACIÓN DE FLUJOS
DE TRABAJO DE DATOS EN LA NUBE. SE TRATA DE UNA HERRAMIENTA DE
EXTRACCIÓN, TRANSFORMACIÓN Y CARGA QUE FACILITA LA INGESTA,
TRANSFORMACIÓN Y MOVIMIENTO DE DATOS DESDE DIVERSAS FUENTES
HASTA DESTINOS COMO ALMACENES DE DATOS, SISTEMAS DE ANÁLISIS
Y SERVICIOS EN LA NUBE.
                 APACHE NIFI
APACHE NIFI ES UN PROYECTO DE CÓDIGO ABIERTO DE LA FUNDACIÓN
APACHE QUE PROPORCIONA UNA PLATAFORMA DE INTEGRACIÓN DE
DATOS Y AUTOMATIZACIÓN DE FLUJOS DE DATOS. NIFI SE ENFOCA EN EL
MOVIMIENTO DE DATOS DESDE MÚLTIPLES FUENTES A TRAVÉS DE UNA
SERIE DE PROCESOS DE TRANSFORMACIÓN Y ENRUTAMIENTO, HASTA
DESTINOS ESPECÍFICOS. LA PRINCIPAL CARACTERÍSTICA DE NIFI ES SU
CAPACIDAD PARA FACILITAR LA AUTOMATIZACIÓN Y GESTIÓN DE FLUJOS
DE DATOS EN TIEMPO REAL Y EN GRANDES VOLÚMENES PROCESADOS
EN BATCH.
 CARACTERÍSTICAS BÁSICAS
TODAS LAS HERRAMIENTAS MENCIONADAS TIENEN CARACTERÍSTICAS
COMUNES ENTRE ELLAS.

NOS PROPORCIONAN UNA INTERFAZ GRÁFICA CON CAJAS QUE
REPRESENTAN PROCESOS, Y QUE PODEMOS CONECTAR PARA MODELAR
EL FLUJO DE DATOS DESDE LA EXTRACCIÓN A LA CARGA, CON TODAS
LAS POSIBLES TRANSFORMACIÓNES.
 CARACTERÍSTICAS BÁSICAS
POSEEN UNA AMPLIA GAMA DE CONECTORES, LO QUE LES PERMITE
CONECTARSE A TODO TIPO DE BASE O REPOSITORIO DE DATOS, TANTO
PARA EXTRAER COMO PARA CARGAR DATOS.

PUEDEN SER UNA HERRAMIENTA EXCLUSIVAMENTE DE ETL, O PUEDEN
TENER MÁS FUNCIONALIDADES O ESAR DENTRO DE UNA SUITE DE
HERRAMIENTAS    CON    OTRAS   FUNCIONALIDADES, NORMALMENTE
ASOCIADAS A LA ANALÍTICA DE DATOS.
  CARACTERÍSTICAS BÁSICAS
DAN LA POSIBILIDAD DE CONVERTIR A CÓDIGO, EL FLUJO DE DATOS QUE
DISEÑAMOS EN LA INTERFAZ GRÁFICA, EN ALGÚN LENGUAJE DE
PROGRAMACIÓN EN PARTICULAR, EN SQL O AMBAS.

MANTIENEN UN REGISTRO DE METADATOS QUE DESCRIBEN LOS DATOS,
LO QUE FACILITA EL SEGUIMIENTO DE LA PROCEDENCIA Y LA CALIDAD DE
LOS DATOS.

SON ESCALABLES PARA MANEJAR GRANDES VOLÚMENES DE DATOS Y
CRECIMIENTO FUTURO DE LA EMPRESA.
 CARACTERÍSTICAS BÁSICAS
PROPORCIONAN CAPACIDADES DE MONITOREO PARA SUPERVISAR EL
PROGRESO DE LOS FLUJOS DE TRABAJO Y ALERTAS EN CASO DE
PROBLEMAS, COMO ERRORES EN LOS DATOS O FALLAS EN LA
EJECUCIÓN.

OFRECEN FUNCIONES DE SEGURIDAD PARA PROTEGER LOS DATOS
DURANTE   SU    MOVIMIENTO Y PROCESAMIENTO, INCLUYENDO
AUTENTICACIÓN Y CIFRADO.

MANTIENEN UN REGISTRO HISTÓRICO DE LAS OPERACIONES DE ETL
REALIZADAS PARA RASTREAR CAMBIOS Y PARA AUDITORÍAS Y
SEGUIMIENTO.
06 - NiFi
               ¿QUÉ ES NIFI?
APACHE NIFI ES UN PROYECTO DE CÓDIGO ABIERTO DE LA FUNDACIÓN
APACHE QUE PROPORCIONA UNA PLATAFORMA DE INTEGRACIÓN DE
DATOS Y AUTOMATIZACIÓN DE FLUJOS DE DATOS. NIFI SE ENFOCA EN EL
MOVIMIENTO DE DATOS DESDE MÚLTIPLES FUENTES A TRAVÉS DE UNA
SERIE DE PROCESOS DE TRANSFORMACIÓN Y ENRUTAMIENTO, HASTA
DESTINOS ESPECÍFICOS. LA PRINCIPAL CARACTERÍSTICA DE NIFI ES SU
CAPACIDAD PARA FACILITAR LA AUTOMATIZACIÓN Y GESTIÓN DE FLUJOS
DE DATOS EN TIEMPO REAL Y EN GRANDES VOLÚMENES PROCESADOS
EN BATCH. ESTÁ PROGRAMADO EN JAVA.
            HISTORIA DE NIFI
NIFI SE ORIGINÓ EN LA AGENCIA DE SEGURIDAD NACIONAL (NSA) DE LOS
ESTADOS UNIDOS Y FUE DESARROLLADO POR EL GRUPO DE
SOLUCIONES DE CÓDIGO ABIERTO (OSSG) DE LA NSA. LA NSA LO
DESARROLLÓ CON EL PROPÓSITO DE GESTIONAR Y AUTOMATIZAR EL
FLUJO DE DATOS EN ENTORNOS COMPLEJOS DE RED Y SEGURIDAD.
           HISTORIA DE NIFI
EN 2014, LA NSA DONÓ APACHE NIFI A LA ASF COMO UN PROYECTO DE
CÓDIGO ABIERTO BAJO LA LICENCIA APACHE LICENSE 2.0. DESDE
ENTONCES, NIFI HA EXPERIMENTADO UN DESARROLLO Y UNA ADOPCIÓN
SIGNIFICATIVOS EN LA COMUNIDAD DE CÓDIGO ABIERTO Y EN LA
INDUSTRIA EN GENERAL.
           HISTORIA DE NIFI
APACHE NIFI SE HA CONVERTIDO EN UNA HERRAMIENTA POPULAR PARA
EL PROCESAMIENTO DE DATOS EN TIEMPO REAL Y LA GESTIÓN DE
FLUJOS DE DATOS EN UNA VARIEDAD DE ENTORNOS, DESDE LA
INGESTIÓN DE DATOS EN SISTEMAS DE BIG DATA HASTA LA
AUTOMATIZACIÓN DE FLUJOS DE TRABAJO EN LA NUBE. SU DISEÑO
MODULAR Y SU CAPACIDAD PARA CONECTAR CON NUMEROSOS
SISTEMAS Y FUENTES DE DATOS LO HACEN ADECUADO PARA UNA
AMPLIA GAMA DE CASOS DE USO EN LA GESTIÓN DE DATOS Y LA
INTEGRACIÓN DE SISTEMAS.
      CARACTERÍSTICAS GENERALES
INTERFAZ GRÁFICA Y ARRASTRAR Y SOLTAR: NIFI OFRECE UNA INTERFAZ
DE USUARIO GRÁFICA Y FÁCIL DE USAR QUE PERMITE A LOS USUARIOS
DISEÑAR   FLUJOS   DE  DATOS    ARRASTRANDO     Y  CONECTANDO
COMPONENTES EN UN LIENZO.

CONECTIVIDAD AMPLIA: PROPORCIONA UNA AMPLIA GAMA DE
CONECTORES PARA INTERACTUAR CON DIVERSAS FUENTES Y DESTINOS
DE DATOS, INCLUIDAS BASES DE DATOS, SERVICIOS WEB, SISTEMAS DE
MENSAJERÍA, SISTEMAS DE ARCHIVOS Y MÁS.
      CARACTERÍSTICAS GENERALES
PROCESAMIENTO EN BATCH Y EN TIEMPO REAL: NIFI ESTÁ DISEÑADO
PARA ADMITIR FLUJOS DE DATOS EN TIEMPO REAL, LO QUE SIGNIFICA
QUE PUEDES MOVER Y TRANSFORMAR DATOS A MEDIDA QUE FLUYEN,
ADEMÁS DE PROCESARLOS POR LOTES.

TRANSFORMACIONES   Y   ENRIQUECIMIENTO: PERMITE  REALIZAR
TRANSFORMACIONES EN LOS DATOS A MEDIDA QUE SE MUEVEN POR
LOS FLUJOS, LO QUE INCLUYE FILTRADO, ENRIQUECIMIENTO,
ENMASCARAMIENTO Y MÁS.
      CARACTERÍSTICAS GENERALES
ORQUESTACIÓN Y ENRUTAMIENTO: SE PUEDEN DISEÑAR FLUJOS DE
DATOS  COMPLEJOS   QUE   INVOLUCREN  MÚLTIPLES ETAPAS DE
PROCESAMIENTO Y ENRUTAMIENTO BASADO EN REGLAS.

PRIORIZACIÓN Y BALANCEO DE CARGA: NIFI PERMITE ESTABLECER
REGLAS DE PRIORIZACIÓN PARA DETERMINAR CÓMO SE DEBEN
GESTIONAR LOS FLUJOS DE DATOS Y CÓMO SE DISTRIBUYEN LAS
CARGAS DE TRABAJO.
      CARACTERÍSTICAS GENERALES

SEGURIDAD Y CONTROL: PROPORCIONA CAPACIDADES DE SEGURIDAD Y
AUTENTICACIÓN PARA PROTEGER LOS DATOS Y GARANTIZAR QUE SOLO
USUARIOS AUTORIZADOS PUEDAN ACCEDER A LOS FLUJOS DE DATOS.

ESCALABILIDAD: NIFI SE ESCALA HORIZONTALMENTE PARA MANEJAR
VOLÚMENES DE DATOS CRECIENTES.
      CARACTERÍSTICAS GENERALES
MONITOREO Y GESTIÓN: OFRECE HERRAMIENTAS PARA MONITOREAR Y
ADMINISTRAR LOS FLUJOS DE DATOS, LO QUE PERMITE IDENTIFICAR Y
RESOLVER PROBLEMAS EN TIEMPO REAL.

EXTENSIBILIDAD Y COMUNIDAD ACTIVA: COMO PROYECTO DE CÓDIGO
ABIERTO BAJO LA FUNDACIÓN APACHE, NIFI CUENTA CON UNA
COMUNIDAD ACTIVA QUE CONTRIBUYE A SU DESARROLLO Y MEJORA
CONSTANTE.    ADEMÁS,   OFRECE    EXTENSIONES  Y   PLUGINS
PERSONALIZADOS.
      CARACTERÍSTICAS GENERALES
MONITOREO Y GESTIÓN: OFRECE HERRAMIENTAS PARA MONITOREAR Y
ADMINISTRAR LOS FLUJOS DE DATOS, LO QUE PERMITE IDENTIFICAR Y
RESOLVER PROBLEMAS EN TIEMPO REAL.

EXTENSIBILIDAD Y COMUNIDAD ACTIVA: COMO PROYECTO DE CÓDIGO
ABIERTO BAJO LA FUNDACIÓN APACHE, NIFI CUENTA CON UNA
COMUNIDAD ACTIVA QUE CONTRIBUYE A SU DESARROLLO Y MEJORA
CONSTANTE.    ADEMÁS,   OFRECE    EXTENSIONES  Y   PLUGINS
PERSONALIZADOS.
                   PROCESSORS
LOS PROCESSORS, SON LA UNIDAD BÁSICA POR LA QUE ESTÁ FORMADA
UN FLUJO DE DATOS. HAY MÁS DE 300 CONECTORES DISTINTOS, CADA
UNO CON SU FUNCIONALIDAD. ALGUNOS PARA CONECTARSE A FUENTES
DE DATOS Y LEER, OTROS PARA CARGAR DATOS A UNA FUENTE, Y EL
RESTO PARA TODAS LAS POSIBLES TRANSFORMACIONES, GESTIÓN DE
LOS DATOS Y METADATOS, Y DISEÑO DEL FLUJO DE DATOS. NIFI ES
CONCURRENTE, PERMITIENDO QUE SE EJECUTEN EN PARALELO VARIOS
FLUJOS DE DATOS. LOS PROCESSORS ABSTRAEN AL USUARIO DE LA
COMPLEJIDAD DE PROGRAMAR EL ETL, Y APORTAN LA SENSACIÓN DE
FLUJO DE DATOS, NO TAN EVIDENTE AL PROGRAMAR.
                   CONNECTORS

LOS PROCESSORS ESTÁN UNIDOS POR CONECTORES (CONNECTORS)
QUE DETERMINAN LA DIRECCIÓN DEL FLUJO DE DATOS Y EL ÓRDEN EN
QUE SE EJECUTAN LOS PROCESSORS. TAMBIÉN DETERMINAN LAS
CAUSISTICAS QUE PERMITEN AVANZAR AL FLUJO. POR EJEMPLO EN
CASO DE FALLA, EXITO, U OTROS.
                        CANVAS


ES EL FONDO BLANCO CUADRICULADO SOBRE EL CUÁL COLOCAMOS,
ARRASTRANDO CON EL MOUSE, LOS DISINTOS PROCESSORS. LA GRILLA
CUADRICULADA SIRVE PARA ORDNEAR, ALINEAR Y PROLIJAR LOS
DIFERENTES PROCESSORS Y SUS RESPECTIVOS CONECTORES.
                PROCESS GROUP

DE FORMA DE ORDENAR LOS DISTINTOS FLUJOS DE DATOS PARA ETL
QUE SE CONSTRUYEN. NIFI PROVEE LOS PROCESS GROUPS. SIRVEN
PARA CONTENER OTROS PROCESS GROUPS, O FLUJOS DE DATOS, Y
MONITOREARLOS ESPECIFICAMENTE, YA QUE NOS DAN INFORMACIÓN
SOBRE SU CONTENIDO.
                     FLOWFILES
FLOWFILES ES EL NOMBRE QUE RECIBE LOS DATOS Y SUS METADATOS
DENTRO DE NIFI. CUANDO UN FLUJO DE DATOS ESTÁ ACTIVO VEMOS
QUE SE VA AGRUPANDO EN FILAS (QUEUE) EN LOS DISTITNTOS
PROCESSORS O CONNECTORS. LA REFERENCIA A LOS DATOS ES EL
CONTENIDO, Y ALGUNOS METADATOS USADOS FRECUENTEMENTE SE
ENCUTRAN BAJO EL NOMBRE DE FILE NAME, FILE PATH, ID. EL
CONTENIDO MISMO NO VIAJA EN EL FLOWFILE SINO UN PUNTERO QUE
HACE REFERENCIA AL MISMO, DENTRO DEL REPOSITORIO DE
CONTENIDO PROPIO DE NIFI.
       REPOSITORIO DE CONTENIDO

EL REPOSITORIO DE CONTENIDO ES SIMPLEMENTE EL LUGAR EN EL
ALMACENAMIENTO LOCAL DONDE RESIDE EL CONTENIDO DE TODOS LOS
FLOWFILES. NO TODOS LOS PROCESADORES NECESITAN ACCEDER AL
CONTENIDO DEL FLOWFILE PARA REALIZAR SUS OPERACIONES, POR
EJEMPLO, LA AGREGACIÓN DEL CONTENIDO DE DOS FLOWFILES NO
REQUIERE CARGAR SU CONTENIDO EN MEMORIA.
        REPOSITORIO DE CONTENIDO

CUANDO UN PROCESADOR MODIFICA EL CONTENIDO DE UN FLOWFILE,
LOS DATOS ANTERIORES SE CONSERVAN. NIFI REALIZA UNA COPIA AL
ESCRIBIR, MODIFICA EL CONTENIDO MIENTRAS LO COPIA A UNA NUEVA
UBICACIÓN. LA INFORMACIÓN ORIGINAL SE MANTIENE INTACTA EN EL
REPOSITORIO DE CONTENIDO.
          PROVENANCE REPOSITORY
NIFI  PROPORCIONA    OTRA   HERRAMIENTA     PARA   REALIZAR  UN
SEGUIMIENTO COMPLETO DE LA HISTORIA DE TODOS LOS FLOWFILES EN
EL FLUJO: EL REPOSITORIO DE EVIDENCIA (PROVENANCE REPOSITORY).
CADA VEZ QUE SE MODIFICA UN FLOWFILE, NIFI TOMA UN SNAPSHOT DEL
FLOWFILE Y SU CONTEXTO EN ESE PUNTO. EL NOMBRE DE ESTE
SNAPSHOT EN NIFI ES UN EVENTO DE EVIDENCIA. EL REPOSITORIO DE
EVIDENCIA REGISTRA EVENTOS DE EVIDENCIA (PROVENANCE EVENTS).
LA EVIDENCIA NOS PERMITE RASTREAR LA TRAZABILIDAD DE LOS DATOS
Y CONSTRUIR LA CADENA COMPLETA DE CUSTODIA PARA CADA PIEZA DE
INFORMACIÓN PROCESADA EN NIFI.
         PROVENANCE VS CONTENT
EL REPOSITORIO DE CONTENIDO ES UN LOG QUE CONTIENE SOLO EL
ESTADO MÁS RECIENTE DE LOS FLOWFILES EN USO EN EL SISTEMA. ES
LA IMAGEN MÁS RECIENTE DEL FLUJO Y PERMITE RECUPERARSE
RÁPIDAMENTE DE UNA INTERRUPCIÓN.

POR OTRO LADO, EL REPOSITORIO DE EVIDENCIA ES MÁS EXHAUSTIVO,
YA QUE REALIZA UN SEGUIMIENTO DEL CICLO DE VIDA COMPLETO DE
CADA FLOWFILE QUE HA ESTADO EN EL FLUJO.
 07 - Motores de
Procesamiento de
      Datos
    COMPONENTES DEL DATALAKE
                                               Visualización y
                                                  Consumo
Fuentes
de Datos   DW
                              Almacenamiento




                              Procesamiento
                ETL


                                Modelado y
                      Usted     Consultas

                      está
                      aquí
                   ¿QUÉ SON?
UN MOTOR DE PROCESAMIENTO DE DATOS ES UNA PARTE FUNDAMENTAL DE
CUALQUIER SISTEMA O PLATAFORMA DISEÑADA PARA EL PROCESAMIENTO Y
ANÁLISIS DE DATOS. ES UN COMPONENTE DE SOFTWARE QUE SE ENCARGA
DE EJECUTAR OPERACIONES ESPECÍFICAS RELACIONADAS CON LA
MANIPULACIÓN, TRANSFORMACIÓN, CONSULTA O ANÁLISIS DE DATOS. ESTOS
MOTORES ESTÁN DISEÑADOS PARA REALIZAR TAREAS DE PROCESAMIENTO
DE DATOS DE MANERA EFICIENTE Y A MENUDO ESTÁN OPTIMIZADOS PARA
TAREAS PARTICULARES.
  TAREAS DE UN MOTOR DE PROCESAMIENTO DE
                         DATOS
PROCESAMIENTO DE DATOS EN BRUTO: ESTO IMPLICA LA CAPACIDAD DE
LEER, ESCRIBIR Y MANIPULAR DATOS EN DIFERENTES FORMATOS Y DESDE
DIVERSAS FUENTES.

TRANSFORMACIÓN DE DATOS: LOS MOTORES DE PROCESAMIENTO DE DATOS
PUEDEN APLICAR TRANSFORMACIONES A LOS DATOS, COMO LA LIMPIEZA DE
DATOS, LA AGREGACIÓN, LA NORMALIZACIÓN, LA UNIÓN DE CONJUNTOS DE
DATOS Y LA CONVERSIÓN DE FORMATOS.
TAREAS DE UN MOTOR DE PROCESAMIENTO DE DATOS

ANÁLISIS DE DATOS: PUEDEN EJECUTAR CONSULTAS Y ANÁLISIS COMPLEJOS
EN LOS DATOS, LO QUE INCLUYE OPERACIONES DE AGREGACIÓN, FILTRADO,
CLASIFICACIÓN Y CÁLCULOS ESTADÍSTICOS.

PROCESAMIENTO EN TIEMPO REAL: ALGUNOS MOTORES DE PROCESAMIENTO
DE DATOS ESTÁN DISEÑADOS ESPECÍFICAMENTE PARA EL PROCESAMIENTO
DE DATOS EN TIEMPO REAL, LO QUE PERMITE ANALIZAR Y TOMAR
DECISIONES BASADAS EN DATOS A MEDIDA QUE LLEGAN.
TAREAS DE UN MOTOR DE PROCESAMIENTO DE DATOS

OPTIMIZACIÓN DE CONSULTAS: LOS MOTORES DE PROCESAMIENTO DE DATOS
A MENUDO ESTÁN OPTIMIZADOS PARA OPTIMIZAR LA EJECUCIÓN DE LAS
CONSULTAS SOBRE LOS DATOS QUE REALIZAN LOS USUARIOS PARA HACER
EL USO MÁS EFICIENTE POSIBLE DE LOS RECURSOS.

PROCESAMIENTO DISTRIBUIDO: MUCHOS MOTORES DE PROCESAMIENTO DE
DATOS ESTÁN DISEÑADOS PARA FUNCIONAR EN CLÚSTERES DE SERVIDORES
PARA DISTRIBUIR LA CARGA DE TRABAJO Y ESCALAR HORIZONTALMENTE.
  ¿SUSTITUYEN A LAS HERRAMIENTAS DE ETL?

ES TEÓRICAMENTE POSIBLE SUSTITUIR CON UNA DE ESTAS
HERRAMIENTAS A UNA HERRAMIENTA DE ETL. TENEMOS LA
POSIBILIDAD DE LEER Y ESCRIBIR DE VARIAS FUENTES DE DATOS
Y REALIZAR TODAS LAS TRANSFORMACIONES DE DATOS QUE SE
NECESITEN, UNA VEZ LOS ARCHIVOS ESTÉN ALMACENADOS EN
NUESTRO SISTEMA DE ARCHIVOS DISTRIBUIDOS.
  ¿SUSTITUYEN A LAS HERRAMIENTAS DE ETL?
SIN EMBARGO NO SE HACE ASÍ. LAS VENTAJAS DE LAS
HERRAMIENTAS DE ETL SON SOBRE TODO A LA HORA DE
PROGRAMAR INGESTAS DE DATOS A NUESTRO SISTEMA DE
ARCHIVOS DISTRIBUIDOS. MIENTRAS QUE POR EL GRAN VOLÚMEN
DE DATOS, ALGUNOS TIPOS DE TRANSFORMACIONES SON MÁS
EFICIENTES SI SON REALIZADOS EN NUESTRO HDFS. ALGUNAS DE
ESTAS TRANSFORMACIONES SON LAS AGRUPACIONES, JOINS
ENTRE TABLAS Y CÁLCULOS DE AGREGACIONES.
  ¿SUSTITUYEN A LAS HERRAMIENTAS DE ETL?
LAS HERRAMIENTAS DE ETL ENTONCES TOMAN EL PAPEL DE
HERRAMIENTA PARA INGESTAR DATOS O EXTRAER COPIAS DE LOS
DATOS DEL DATALAKE. LAS INGESTAS DE LOS DATOS SON DESDE
SU FUENTE ORIGINAL O UNA COPIA DE LA MISMA. Y LA
EXTRACCIÓN DE DATOS DESDE EL DATALAKE ES DE DATOS YA
REFINADOS Y ENRIQUECIDOS A PARTIR DE VARIAS FUENTES PARA
SER USADOS POR MIEMBROS DE LA ORGANIZACIÓN U OTROS
SISTEMAS.
¿CÓMO USAMOS LOS MOTORES DE PROCESAMIENTO DE DATOS?

UNA VEZ LOS DATOS SON INGESTADOS AL DATALAKE, PODEMOS
PROCESARLOS   PARA   SU  REFINADO.   COMO   EXPLICAMOS
ANTERIORMENTE, NO TODAS LAS TRANSFORMACIONES SON
REALIZADAS CON LAS HERRAMIENTAS DE ETL. EL PRIMER PASO
ES GUARDAR UNA COPIA DENTRO DEL DATALAKE DEL ARCHIVO EN
SU FORMATO ORIGINAL. LUEGO SI SE REALIZAN TODAS LAS
TRANSFORMACIONES FALTANTES.
¿CÓMO USAMOS LOS MOTORES DE PROCESAMIENTO DE DATOS?



ESTE PRIMER PROCESO SE LO CONOCE COMO REFINADO DE
DATOS.   SE   APLICA   SOBRE   DATOS    ESTRUCTURADOS,
LLEVANDOLOS A UN FORMATO TABULAR QUE PERMITA LA
INTEGRACIÓN DE DATOS PROVENIENTES DE DISTINTAS FUENTES.
¿CÓMO USAMOS LOS MOTORES DE PROCESAMIENTO DE DATOS?

LUEGO DE REFINADO DE DATOS DEBEMOS ELEGIR QUE FORMA DE
MODELAR LOS DATOS UTILIZAREMOS. VAMOS A TENER QUE
ADAPTAR LOS DISTINTOS ARCHIVOS CON DATOS PARA LA
ESTRUCTURA     QUE    UTILICEMOS.  LUEGO    DE    ESTE
PROCESAMIENTO, NUESTROS ARCHIVOS QUEDAN PRONTOS PARA
SER CONSUMIDOS POR HERRAMIENTAS DE CONSULTA DE DATOS
EN FORMATO TABLULAR.
 ¿CÓMO USAMOS LOS MOTORES DE PROCESAMIENTO DE DATOS?
UNA VEZ LOS DATOS ESTÉN SIENDO CONSUMIDOS A TRAVÉS DE
TABLAS, LOS ANALÍSTAS DE DATOS PUEDEN HACER CONSULTAS
SOBRE LOS MISMOS. LOS MOTORES DE PROCESAMIENTO DE
DATOS, TAMBIÉN PUEDEN SER UTILIZADOS PARA HACER
CONSULTAS SOBRE ESTAS TABLAS, PUDIENDO TAMBIÉN ARMAR
PROCESOS QUE LUEGO SON AUTOMATIZADOS UNA VEZ
TENGAMOS ARMADO UN ANÁLISIS QUE QUIERAMOS DEJAR
PRONTO PARA SER UTILIZADO POR EL RESTO DE LA
ORGANIZACIÓN.
¿CÓMO USAMOS LOS MOTORES DE PROCESAMIENTO DE DATOS?



CON ESTOS PROCESOS QUE ARMAMOS PODEMOS CONSTRUIR
DATAMARTS (CONJUNTOS DE DATOS RELATIVOS A UN TEMA EN
PARTICULAR) O PODEMOS DEJAR PRONTO UN ANÁLISIS FINAL QUE
VAYA A SER EXTRAÍDO DEL DATALAKE.
08 - Spark
                      ¿QUÉ ES
                      SPARK?
APACHE SPARK ES UN FRAMEWORK DE PROCESAMIENTO DE DATOS EN
CÓDIGO ABIERTO QUE SE UTILIZA PARA EL PROCESAMIENTO Y ANÁLISIS DE
GRANDES VOLÚMENES DE DATOS DE MANERA EFICIENTE Y ESCALABLE. FUE
DESARROLLADO INICIALMENTE EN LA UNIVERSIDAD DE CALIFORNIA,
BERKELEY, Y DESDE ENTONCES HA GANADO UNA AMPLIA ADOPCIÓN EN LA
INDUSTRIA Y EN LA COMUNIDAD DE CÓDIGO ABIERTO.
                       ¿QUÉ ES
                       SPARK?
APACHE SPARK FUE CREADO EN LA UNIVERSIDAD DE CALIFORNIA, BERKELEY,
EN EL LABORATORIO DE AMPLAB (LABORATORIO DE ANÁLISIS DE DATOS EN
MEMORIA DE BERKELEY). EL PROYECTO FUE INICIADO POR MATEI ZAHARIA EN
2009 COMO PARTE DE SU TESIS DOCTORAL. MATEI ZAHARIA ES UN CIENTÍFICO
DE DATOS Y PROGRAMADOR RUMANO-CANADIENSE QUE LIDERÓ EL
DESARROLLO INICIAL DE SPARK Y CONTRIBUYÓ SIGNIFICATIVAMENTE A SU
DISEÑO Y ARQUITECTURA.
                      ¿QUÉ ES
                      SPARK?
PARA EL AÑO 2010 EL CÓDIGO DE SPARK SE PUBLICÓ COMO OPEN SOURCE, Y
PARA EL AÑO 2013 YA FORMABA PARTE DE LA FUNDACIÓN APACHE Y EN 2014
LA APACHE FOUNDATION LE DIÓ LA CATEGORÍA DE “TOP LEVEL PROJECT”.
DESDE ENTONCES SE CONVITRIÓ EN UNO DE LOS MÁS POPULARES
FRAMEWORKS PARA PROCESAMIENTO DE DATOS EN LA INDUSTRIA. EL
OBJETIVO DE SPARK ES PROVEER UN FRAMEWORK DISTRIBUIDO, DE
PROPÓSITO GENERAL, PARA PROCESAR GRANDES VOLÚMENES DE DATOS.
DISEÑADO PARA SUPERAR LAS LIMITACIONES DE MAPREDUCE, EL
FRAMEWORK DE PROCESAMIENTO DE BIGDATA NATIVO DE HADOOP.
                     ¿QUÉ ES
                     SPARK?
SPARK SE DESTACA POR SU CAPACIDAD PARA REALIZAR OPERACIONES DE
PROCESAMIENTO EN MEMORIA, LO QUE SIGNIFICA QUE PUEDE CARGAR
DATOS EN LA MEMORIA RAM DE UN CLÚSTER DE SERVIDORES Y REALIZAR
OPERACIONES DE PROCESAMIENTO DE DATOS EN TIEMPO REAL DE MANERA
MUY RÁPIDA.
                      ¿QUÉ ES
                      SPARK?
SPARK PROPORCIONA UN FRAMEWORK UNIFICADO Y DE PROPÓSITO
GENERAL PARA REALIZAR DIFERENTES TAREAS QUE ANTERIORMENTE HABRÍA
REQUERIDO DIFERENTES MOTORES, CÓMO EL PROCESAMIENTO EN BATCH O
EN TIEMPO REAL.
                      ¿QUÉ ES
ALGUNAS DE SUS CARACTERÍSTICAS SON:

                      SPARK?
PROCESAMIENTO EN MEMORIA: SPARK ALMACENA DATOS EN MEMORIA, LO
QUE ACELERA SIGNIFICATIVAMENTE EL PROCESAMIENTO EN COMPARACIÓN
CON SOLUCIONES DE PROCESAMIENTO EN DISCO COMO HADOOP
MAPREDUCE.

SOPORTE PARA MÚLTIPLES LENGUAJES: SPARK OFRECE APIS EN VARIOS
LENGUAJES DE PROGRAMACIÓN, COMO SCALA, JAVA, PYTHON Y R, LO QUE
FACILITA SU ADOPCIÓN POR PARTE DE DESARROLLADORES CON DIFERENTES
HABILIDADES.
                       ¿QUÉ ES
                       SPARK?
LIBRERÍAS EXTENSAS: SPARK CUENTA CON UN ECOSISTEMA DE LIBRERÍAS
ADICIONALES, COMO SPARK SQL (PARA PROCESAMIENTO SQL), SPARK
STREAMING (PARA PROCESAMIENTO EN TIEMPO REAL), MLLIB (PARA MACHINE
LEARNING) Y GRAPHX (PARA PROCESAMIENTO DE GRÁFOS), LO QUE LO HACE
VERSÁTIL PARA UNA VARIEDAD DE APLICACIONES.

ESCALABILIDAD: PUEDE ESCALAR HORIZONTALMENTE PARA MANEJAR
GRANDES VOLÚMENES DE DATOS Y CARGAS DE TRABAJO INTENSIVAS.
                     ¿QUÉ ES
INTEGRACIÓN CON SISTEMAS DE ALMACENAMIENTO DE DATOS: SE INTEGRA
                     SPARK?
CON VARIOS SISTEMAS DE ALMACENAMIENTO DE DATOS, INCLUYENDO
HADOOP HDFS, APACHE HBASE Y BASES DE DATOS RELACIONALES, LO QUE
FACILITA LA INGESTIÓN Y EXPORTACIÓN DE DATOS HACIA Y DESDE UN
DATALAKE.

MODELO DE PROGRAMACIÓN UNIFICADO: SPARK OFRECE UN MODELO DE
PROGRAMACIÓN UNIFICADO QUE PERMITE A LOS DESARROLLADORES
ESCRIBIR CÓDIGO UNA VEZ Y EJECUTARLO TANTO EN TAREAS DE
PROCESAMIENTO EN BATCH COMO EN STREAMING.
                       ¿QUÉ ES
COMUNIDAD ACTIVA: SPARK CUENTA CON UNA COMUNIDAD DE DESARROLLO
                       SPARK?
ACTIVA Y ES RESPALDADO POR ORGANIZACIONES LÍDERES EN TECNOLOGÍA,
LO QUE GARANTIZA UN DESARROLLO CONTINUO Y UN SÓLIDO SOPORTE.
(DATABRICKS Y APACHE SOFTWARE FOUNDATION)

COMPATIBILIDAD CON LA NUBE: PUEDE EJECUTARSE EN PLATAFORMAS DE
NUBE COMO AMAZON WEB SERVICES (AWS), GOOGLE CLOUD PLATFORM
(GCP) Y MICROSOFT AZURE, LO QUE FACILITA LA ADOPCIÓN EN ENTORNOS DE
NUBE E INTEGRACIÓN CON LOS SERVICIOS NATIVOS QUE OFRECEN LAS
MISMAS.
      SPARK VS MAPREDUCE

SPARK PROPORCIONA OPERADORES DE ALTO NIVEL COMO MAP, FILTROS,
ETC., QUE FACILITA EL TRABAJO DEL DESARROLLADOR. SI UTILIZARAN
DIRECTAMENTE MAPREDUCE, LOS DESARROLLADORES DEBERÍAN CODIFICAR
A MANO CADA OPERACIÓN.
      SPARK VS MAPREDUCE
SPARK ES 100 VECES MÁS RÁPIDO QUE MAPREDUCE CUANDO SE EJECUTA
SPARK EN MEMORIA Y 10 VECES MÁS RÁPIDO QUE MAPREDUCE CUANDO SE
EJECUTA SPARK EN DISCO AL REDUCIR LA CANTIDAD DE CICLOS DE LECTURA
Y ESCRITURA EN EL DISCO Y ALMACENAR DATOS INTERMEDIOS EN LA
MEMORIA.
MAPREDUCE LEE Y ESCRIBE DESDE EL DISCO, LO QUE RALENTIZA LA
VELOCIDAD DE PROCESAMIENTO Y LA EFICIENCIA GENERAL.
     SPARK VS MAPREDUCE

SPARK PROPORCIONA UN SHELL INTERACTIVO PARA APRENDER, HACER
PRUEBAS Y EXPLORAR LOS DATOS.
MAPREDUCE NO PROPORCIONA NINGÚN SHELL INTERACTIVO.
                            RDD
LA BASE DE SPARK SON LOS RDD (RESILIENT DISTRIBUTED DATASET). UN RDD
ES UNA ABSTRACCIÓN DE COLECCIÓN DE OBJETOS DE SOLO LECTURA,
DISTRIBUIDOS EN UN CLÚSTER DE SERVIDORES. ES TOLERANTE A FALLOS Y
ESTÁ COMPUESTO POR ELEMENTOS QUE SE PUEDEN PROCESAR EN
PARALELO. LOS RDDS SE UTILIZAN EN SPARK PARA REPRESENTAR
CONJUNTOS DE DATOS DISTRIBUIDOS QUE SE PUEDEN DIVIDIR Y PROCESAR
EN MÚLTIPLES NODOS DE UN CLÚSTER DE MANERA PARALELA. SE PUEDE
CREAR UN RDD A PARTIR DE UN ARCHIVOS DE TEXTO, BASES DE DATOS SQL,
O NOSQL, ARCHIVOS EN HDFS O ARCHIVOS GUARDADOS EN LA NUBE, ENTRE
OTROS.
     CARACTERÍSTICAS RDD
DISTRIBUCIÓN Y PARALELISMO: LOS RDDS PERMITEN QUE LOS DATOS SE
DISTRIBUYAN EN MÚLTIPLES NODOS DE UN CLÚSTER SPARK Y SE PROCESEN
EN PARALELO. ESTO FACILITA EL PROCESAMIENTO EFICIENTE DE GRANDES
VOLÚMENES DE DATOS.

INMUTABILIDAD: LOS RDDS SON INMUTABLES, LO QUE SIGNIFICA QUE UNA
VEZ QUE SE CREA UN RDD, NO SE PUEDEN MODIFICAR. SIN EMBARGO, SE
PUEDEN REALIZAR TRANSFORMACIONES EN UN RDD PARA CREAR UNO
NUEVO.
     CARACTERÍSTICAS RDD
TOLERANCIA A FALLOS: LOS RDDS SON TOLERANTES A FALLOS. ESTO
SIGNIFICA QUE SI UN NODO EN EL CLÚSTER FALLA DURANTE UN CÁLCULO,
SPARK PUEDE RECONSTRUIR AUTOMÁTICAMENTE LOS DATOS PERDIDOS EN
OTRO NODO Y CONTINUAR EL PROCESAMIENTO.

OPERACIONES: LOS RDDS ADMITEN DOS TIPOS DE OPERACIONES:
TRANSFORMACIONES    Y ACCIONES. LAS  TRANSFORMACIONES    SON
OPERACIONES QUE CREAN UN NUEVO RDD A PARTIR DE UNO EXISTENTE,
MIENTRAS QUE LAS ACCIONES DEVUELVEN RESULTADOS O VALORES AL
PROGRAMA PRINCIPAL.
     CARACTERÍSTICAS RDD
LOS RDD PERMITEN REALIZAR FUNCIONES DE MAPREDUCE PERO TAMBIÉN,
OPERACIONES DE JOINS ENTRE DATASETS, FILTRADO Y AGREGACIONES.
TODO EL PROCESAMIENTO DE LOS RDD ES HECHO EN MEMORIA. LA
COMPLEJIDAD DEL RDD ES ESCONDIDA AL USUARIO FINAL PARA QUE NO
TENGA QUE PREOCUPARSE MÁS QUE DE LAS OPERACIONES QUE QUIERE
REALIZAR SOBRE LOS DATOS. SUPERANDO LA FORMA TRADICIONAL DE
PROGRAMAR TAREAS DE MAPREDUCE.
       PROCESAMIENTO RDD
EL PROCESAMIENTO DE UN RDD SE HACE MEDIANTE DRIVERS Y EXECUTORS.
AL EJECUTAR UN PROGRAMA UN DRIVER CREA UN SPARK CONTEXT. EL SPARK
CONTEXT ES UN ORQUESTADOR QUE LEE EL CÓDIGO Y CONSIDERA EL
TRABAJO QUE DEBE REALIZARSE. EL TRABAJO A REALIZAR SE DIVIDE EN
VARIOS “TASKS”. GENERA UN PLAN FÍSICO Y UTILIZA EL CLUSTER MANAGER
PARA COORDINAR QUE EXECUTORS VAN A AGENDAR Y EJECUTAR CUALES
TASKS. EL SCHEDULER UTILIZADO SE ENCUENTRA DENTRO DEL CLUSTER
MANAGER, Y SE LLAMA DAG SCHEDULER. LA SIGLA DAG VIENE DE GRÁFO
DIRIGIDO ACÍCLICO. LOS DISTINTOS TASKS DE SPARK SE VAN A EJECUTAR DE
ESTA FORMA. EL DAG SCHEDULER ASIGNA LOS TASKS A LOS “WORKER
NODES”.
       PROCESAMIENTO RDD
LOS “WORKER NODES” SON LOS SERVIDORES QUE CONTIENEN A LOS
EXECUTORS. UNA VEZ FINALIZADO EL TASK, EL EXECUTOR DEVUELVE EL
RESULTADO DE SU TAREA AL DRIVER, EL CUÁL JUNTA TODOS LOS
RESULTADOS PARA EL OUTPUT DEL PROGRAMA DE SPARK.
                                           WORKER NODE 1


                                             EXECUTOR

                                           TASK   TASK




                         CLUSTER MANAGER


DRIVER   SAPRK CONTEXT     DAG SHEDULER




                                           WORKER NODE N


                                             EXECUTOR

                                           TASK   TASK
                    DRIVERS
• ES EL LUGAR DONDE SE CREA SPARKCONTEXT.
• ES EL PUNTO DE ENTRADA PARA EL SHELL INTERACTIVO, QUE ESTÁ
  DISPONIBLE SOLO PARA SCALA, PYTHON Y R.
• EJECUTA LA FUNCIÓN PRINCIPAL (MAIN) DE LA APLICACIÓN.
• ES RESPONSABLE DE PROGRAMAR TRABAJOS Y ASIGNAR RECURSOS
  PARA EJECUTAR TAREAS.
• ES RESPONSABLE DE CONVERTIR LAS APLICACIONES DE USUARIO EN
  UNIDADES DE EJECUCIÓN MÁS PEQUEÑAS CONOCIDAS COMO TAREAS
  (TASK).
               SPARK SESSION
CUANDO REALMENTE COMENCEMOS A ESCRIBIR NUESTRA APLICACIÓN
SPARK, NECESITAREMOS UNA FORMA DE ENVIAR COMANDOS Y DATOS.
LO HACEMOS CREANDO PRIMERO UNA SPARKSESSION.
PARA ACCEDER A LA CONSOLA DE SCALA E INICIAR UNA SESIÓN INTERACTIVA,
EJECUTAMOS SPARK-SHELL Y PARA INICIAR LA CONSOLA DE PYTHON
EJECUTAMOS PYSPARK. ESTO INICIA UNA APLICACIÓN SPARK INTERACTIVA.
              SPARK SESSION
TAMBIÉN EXISTE UN PROCESO PARA ENVIAR APLICACIONES INDEPENDIENTES
A SPARK LLAMADO SPARK-SUBMIT, MEDIANTE EL CUAL PUEDE ENVIAR UNA
APLICACIÓN PRECOMPILADA A SPARK.
CUANDO INICIAMOS SPARK EN ESTE MODO INTERACTIVO, IMPLÍCITAMENTE
CREAMOS UNA SPARKSESSION QUE ADMINISTRA LA APLICACIÓN SPARK.
CUANDO INICIAMOS A TRAVÉS DE UN SUBMIT DE TRABAJO, DEBEMOS
CREARLO O ACCEDER A SPARKSESSION.
              SPARK SESSION
LA INSTANCIA DE SPARKSESSION ES LA FORMA EN QUE SPARK EJECUTA
COMANDOS DEFINIDOS POR EL USUARIO EN TODO EL CLÚSTER.
EXISTE UNA CORRESPONDENCIA UNO A UNO ENTRE UNA SPARKSESSION Y
UNA APLICACIÓN SPARK.
EN SCALA Y PYTHON, LA VARIABLE ESTÁ DISPONIBLE COMO SPARK CUANDO
INICIA LA CONSOLA.
                 EXECUTORS
• SON   RESPONSABLES    DE   EJECUTAR  TAREAS  Y   REALIZAR   EL
  PROCESAMIENTO DE DATOS.
• LEEN Y ESCRIBEN EN FUENTES EXTERNAS.
• ALMACENAN RESULTADOS INTERMEDIOS EN MEMORIA.
        CLUSTER MANAGER

• ES UN SERVICIO EXTERNO.
• ES RESPONSABLE DE ADQUIRIR RECURSOS COMO CPU, MEMORIA Y MÁS Y
  ASIGNARLOS A APLICACIONES SPARK.
         TIPOS DE CLUSTER
• STANDALONE: UN ADMINISTRADOR DE CLÚSTER SIMPLE INCLUIDO CON
             MANAGER
  SPARK QUE FACILITA LA CONFIGURACIÓN DE UN CLÚSTER.
• APACHE MESOS: UN ADMINISTRADOR GENERAL DE CLÚSTERES QUE
  TAMBIÉN PUEDE EJECUTAR HADOOP MAPREDUCE Y APLICACIONES DE
  SERVICIO. (OBSOLETO ACTUALMENTE)
• HADOOP YARN: EL ADMINISTRADOR DE RECURSOS EN HADOOP 2 Y 3.
• KUBERNETES: UN SISTEMA DE CÓDIGO ABIERTO PARA AUTOMATIZAR LA
  IMPLEMENTACIÓN, EL ESCALADO Y LA GESTIÓN DE APLICACIONES EN
  CONTENEDORES.
    MODOS DE EJECUCIÓN
• UN MODO DE EJECUCIÓN DA EL PODER DE DETERMINAR DÓNDE SE
  ENCUENTRAN FÍSICAMENTE LOS RECURSOS CUANDO SE EJECUTA UNA
  APLICACIÓN.
• EXISTEN TRES MODOS PARA ELEGIR:
• MODO CLÚSTER
• MODO CLIENTE
• MODO LOCAL
      MODOS DE EJECUCIÓN:
EL MODO CLUSTER ES PROBABLEMENTE LA FORMA MÁS COMÚN DE
EJECUTAR APLICACIONES SPARK.
           CLUSTER
EN ESTE MODO, UN USUARIO ENVÍA UN JAR PRECOMPILADO, UNA SECUENCIA
DE COMANDOS DE PYTHON O UNA SECUENCIA DE COMANDOS R A UN
ADMINISTRADOR DE CLÚSTERES.
LUEGO, EL ADMINISTRADOR DEL CLUSTER INICIA EL PROCESO DEL DRIVER EN
UN NODO WORKER DENTRO DEL CLUSTER, ADEMÁS DE LOS PROCESOS
EJECUTORES.
ESTO SIGNIFICA QUE EL ADMINISTRADOR DEL CLUSTER ES RESPONSABLE DE
MANTENER TODOS LOS PROCESOS RELACIONADOS CON LA APLICACIÓN
SPARK.
      MODOS DE EJECUCIÓN:
           CLIENTE
EL MODO DE CLIENTE ES CASI IGUAL QUE EL MODO CLUSTER, EXCEPTO QUE
EL DRIVER SPARK PERMANECE EN LA MÁQUINA CLIENTE QUE ENVIÓ LA
APLICACIÓN.
ESTO SIGNIFICA QUE LA MÁQUINA CLIENTE ES RESPONSABLE DE MANTENER
EL PROCESO DEL DRIVER SPARK Y EL ADMINISTRADOR DEL CLUSTER
MANTIENE LOS PROCESOS EJECUTORES.
ESTAS MÁQUINAS SE CONOCEN COMÚNMENTE COMO MÁQUINAS GATEWAY O
NODOS EDGES.
      MODOS DE EJECUCIÓN:
            LOCAL
EL MODO LOCAL ES UNA DESVIACIÓN SIGNIFICATIVA DE LOS DOS MODOS
ANTERIORES: EJECUTA TODA LA APLICACIÓN SPARK EN UNA SOLA MÁQUINA.
LOGRA EL PARALELISMO A TRAVÉS DE SUBPROCESOS EN ESA ÚNICA
MÁQUINA. ESTA ES UNA FORMA COMÚN DE APRENDER SPARK, PROBAR SUS
APLICACIONES O EXPERIMENTAR ITERATIVAMENTE CON EL DESARROLLO
LOCAL.
SIN EMBARGO, NO ES RECOMENDABLE USAR EL MODO LOCAL PARA
EJECUTAR APLICACIONES DE PRODUCCIÓN.
    PROCESAMIENTO SPARK
SPARK PUEDE CORRER ARRIBA DE VARIOS MOTORES DE PROCESAMIENTO.
POR EJEMPLO SOBRE YARN DE HADOOP, APACHE MESOS, KUBERNETES, O
DOCKER SWARM.

EN CASO DE ESTAR TRABAJANDO EN LA NUBE SE PUEDE UTILIZAR UNA
VERSIÓN YA GESTIONADA POR LA NUBE. PARA AWS TENEMOS EMR, PARA
AZURE TENEMOS HDINSIGHTS, Y GOOGLE DATAPROC EN GCP. TAMBIÉN
CONTAMOS CON DATABRICKS, EMPRESA DE LOS CREADORES DE SPARK QUE
ES OFRECIDO COMO SERVICIO CLOUD EN TODAS LAS NUBES.
   INTERACCIÓN CON SPARK
LOS RDD SIGUEN SIENDO LA PIEZA FUNDAMENTAL DE SPARK, PERO SALVO
DESARROLLOS DE SPARK EN VERSIONES VIEJAS, YA NO SON UTILIZADOS.
EXISTEN ABSTRACCIONES DE MÁS ALTO NIVEL COMO LOS DATAFRAMES DE
SPARK QUE SON MÁS AMIGABLES CON EL USUARIO EN CUANTO A SU USO.
ADEMÁS EXISTEN MODULOS DE LIBRERÍAS PARA FACILITAR EL USO DE SPARK
A LOS USUARIOS.
INTERACCIÓN CON SPARK
                DATA FRAMES
UN DATAFRAME ES LA API ESTRUCTURADA MÁS COMÚN Y SIMPLEMENTE
REPRESENTA UNA TABLA DE DATOS CON FILAS Y COLUMNAS. LA LISTA DE
COLUMNAS Y LOS TIPOS EN ESAS COLUMNAS ES EL SCHEMA.

UNA ANALOGÍA SIMPLE SERÍA UNA HOJA DE CÁLCULO CON COLUMNAS CON
NOMBRE. LA DIFERENCIA FUNDAMENTAL ES QUE MIENTRAS UNA HOJA DE
CÁLCULO SE ENCUENTRA EN UNA COMPUTADORA EN UNA UBICACIÓN
ESPECÍFICA, UN SPARK DATAFRAME PUEDE ABARCAR CIENTOS DE
COMPUTADORAS.
                 DATA FRAMES
EL CONCEPTO DATAFRAME NO ES EXCLUSIVO DE SPARK. R Y PYTHON TIENEN
CONCEPTOS SIMILARES. SIN EMBARGO, LOS DATAFRAMES DE PYTHON/R
(CON ALGUNAS EXCEPCIONES) EXISTEN EN UNA SOLA MÁQUINA EN LUGAR DE
EN VARIAS MÁQUINAS.

DADO QUE SPARK TIENE INTERFACES DE LENGUAJE TANTO PARA PYTHON
COMO PARA R, ES MUY FÁCIL CONVERTIR PANDAS (PYTHON) DATAFRAMES A
SPARK DATAFRAMES Y R DATAFRAMES A SPARK DATAFRAMES (EN R).
DATA FRAMES: PARTICIONES
PARA PERMITIR QUE CADA EJECUTOR REALICE EL TRABAJO EN PARALELO,
SPARK DIVIDE LOS DATOS EN FRAGMENTOS, LLAMADOS PARTICIONES.

UNA PARTICIÓN ES UNA COLECCIÓN DE FILAS QUE SE ENCUENTRAN EN UNA
MÁQUINA FÍSICA EN NUESTRO CLUSTER. LAS PARTICIONES DE UN
DATAFRAME REPRESENTAN CÓMO LOS DATOS SE DISTRIBUYEN FÍSICAMENTE
EN SU GRUPO DE MÁQUINAS DURANTE LA EJECUCIÓN.
DATA FRAMES: PARTICIONES
UNA COSA IMPORTANTE A TENER EN CUENTA ES QUE CON DATAFRAMES, NO
MANIPULAMOS (EN SU MAYOR PARTE) LAS PARTICIONES INDIVIDUALMENTE.
SIMPLEMENTE ESPECIFICAMOS TRANSFORMACIONES DE DATOS DE ALTO
NIVEL EN LAS PARTICIONES FÍSICAS Y SPARK DETERMINA CÓMO SE
EJECUTARÁ REALMENTE ESTE TRABAJO EN EL CLÚSTER.
        TRANSFORMACIONES
EN SPARK, LAS ESTRUCTURAS DE DATOS CENTRALES SON INMUTABLES, LO
QUE SIGNIFICA QUE NO SE PUEDEN CAMBIAR UNA VEZ CREADAS.
ESTO PUEDE PARECER UN CONCEPTO EXTRAÑO AL PRINCIPIO, SI NO PUEDE
CAMBIARLO, ¿CÓMO SE SUPONE QUE DEBE USARSE?
PARA CAMBIAR UN DATAFRAME, SE DEBERÁ INDICARLE A SPARK CÓMO NOS
GUSTARÍA MODIFICAR EL DATAFRAME QUE SE TIENE EN EL QUE SE DESEA.
ESTAS INSTRUCCIONES SE LLAMAN TRANSFORMACIONES.
       TRANSFORMACIONES
LAS TRANSFORMACIONES NO DEVUELVEN NINGÚN RESULTADO. ESTO SE
DEBE A QUE ESPECIFICAMOS SOLO UNA TRANSFORMACIÓN ABSTRACTA, Y
SPARK NO ACTUARÁ SOBRE LAS TRANSFORMACIONES HASTA QUE LLAMEMOS
UNA ACCIÓN.

HAY DOS TIPOS DE TRANSFORMACIONES: LAS QUE ESPECIFICAN
DEPENDENCIAS ESTRECHAS (NARROW DEPENDENCIES) Y LAS QUE
ESPECIFICAN DEPENDENCIAS AMPLIAS (WIDE DEPENDENCIES).
       TRANSFORMACIONES:
            NARROW
NARROW TRANSFORMATION: CONSISTEN EN DEPENDENCIAS EN LAS QUE
CADA PARTICIÓN DE ENTRADA CONTRIBUIRÁ A UNA ÚNICA PARTICIÓN DE
SALIDA. ALGUNOS EJEMPLOS SON LAS FUNCIONES FILTER(), SAMPLE(), MAP()
O FLATMAP().
 TRANSFORMACIONES: WIDE
WIDE TRANSFORMATION: SE UTILIZA CUANDO LA LÓGICA DE LA APLICACIÓN
NECESITA DATOS QUE SE ENCUENTRAN EN DIFERENTES PARTICIONES Y ES
NECESARIO MEZCLAR DICHAS PARTICIONES PARA AGRUPAR LOS DATOS
NECESARIOS EN UN DATAFRAME DETERMINADO.

EJEMPLOS DE FUNCIONES: GROUPBYKEY() O REDUCEBYKEY().
            LAZY EVALUATION
LAZY EVALUATION (EVALUACIÓN PEREZOSA) SIGNIFICA QUE SPARK
ESPERARÁ HASTA EL ÚLTIMO MOMENTO PARA EJECUTAR EL GRÁFICO DE
INSTRUCCIONES DE CÁLCULO. EN SPARK, EN LUGAR DE MODIFICAR LOS
DATOS INMEDIATAMENTE CUANDO EXPRESAMOS ALGUNA OPERACIÓN,
CONSTRUIMOS UN PLAN DE TRANSFORMACIONES QUE SE APLICARÁN A
NUESTROS DATOS DE ORIGEN.

ESTO BRINDA INMENSOS BENEFICIOS PARA EL USUARIO FINAL PORQUE
SPARK PUEDE OPTIMIZAR TODO EL FLUJO DE DATOS DE PRINCIPIO A FIN.
            LAZY EVALUATION
SI CONSTRUIMOS UN GRAN TRABAJO DE SPARK PERO ESPECIFICAMOS UN
FILTRO AL FINAL QUE SOLO REQUIERE QUE OBTENGAMOS UNA FILA DE
NUESTROS DATOS DE ORIGEN, LA FORMA MÁS EFICIENTE DE EJECUTAR ESTO
ES ACCEDER AL ÚNICO REGISTRO QUE NECESITAMOS.

SPARK REALMENTE OPTIMIZARÁ ESTO, EMPUJANDO EL FILTRO HACIA ABAJO
AUTOMÁTICAMENTE.
                    ACCIONES
LAS TRANSFORMACIONES NOS PERMITEN CONSTRUIR NUESTRO PLAN
LÓGICO DE TRANSFORMACIÓN. PARA ACTIVAR EL CÁLCULO, EJECUTAMOS
UNA ACCIÓN. UNA ACCIÓN LE INDICA A SPARK QUE CALCULE UN RESULTADO
DE UNA SERIE DE TRANSFORMACIONES.

LA ACCIÓN MÁS SIMPLE ES CONTAR, QUE NOS DA EL NÚMERO TOTAL DE
REGISTROS EN EL DATAFRAME.
                    ACCIONES
HAY TRES TIPOS DE ACCIONES:
 • ACCIONES PARA VER DATOS EN LA CONSOLA.
 • ACCIONES PARA RECOPILAR DATOS A OBJETOS NATIVOS EN EL LENGUAJE
   USADO.
 • ACCIONES PARA ESCRIBIR DATOS DE SALIDA.
                      SPARK UI
DURANTE LA EJECUCIÓN DE SPARK, LOS USUARIOS PUEDEN MONITOREAR EL
PROGRESO DE SU TRABAJO A TRAVÉS DE LA INTERFAZ DE USUARIO DE
SPARK.
LA INTERFAZ DE USUARIO DE SPARK ESTÁ DISPONIBLE EN EL PUERTO 4040
DEL NODO DRIVER. SI ESTÁ EJECUTANDO EN MODO LOCAL, SÓLO SERÁ EL
HTTP://LOCALHOST:4040.

LA INTERFAZ DE USUARIO DE SPARK MANTIENE INFORMACIÓN SOBRE EL
ESTADO DE LOS TRABAJOS, EL ENTORNO Y EL ESTADO DEL CLUSTER DE
SPARK. ES MUY ÚTIL, ESPECIALMENTE PARA AJUSTAR Y DEPURAR.
    DESVENTAJAS DE SPARK
LA GRAN DESVENTAJA DE SPARK ES LA GRAN CANTIDAD DE MEMORIA RAM
QUE REQUIERE PARA TRABAJAR. ES IMPORTANTE CUIDAR LOS RECURSOS
QUE UTILIZA UN PROCESO DE SPARK Y VERIFICAR SI NO PUEDE
PROGRAMARSE DE MEJOR FORMA CUANDO UN PROCESO CONSUME UNA
GRAN CANTIDAD DE RECURSOS. ESTO PODRÍA PERJUDICAR QUE CORRAN
OTROS PROCESOS EN EL DATALAKE SI ESTAMOS ON PREMISE, Y HACERNOS
INCURRIR EN MAYORES COSTOS SI ESTAMOS EN LA NUBE.
  09 -
Modelado
de Datos
          ¿DONDE ESTAMOS?
HASTA AHORA INGESTAMOS DATOS DE DISTINTAS FUENTES AL DATALAKE.
TANTO DATOS ESTRUCTURADOS COMO NO ESTRUCTURADOS. LOS DATOS
ESTRUCTURADOS, POR EJEMPLO LOS QUE PROVIENEN DE BASES DE DATOS,
TRATAMOS CADA TABLA COMO UN ARCHIVO. LUEGO DE LA INGESTA HACEMOS
CON SPARK LAS TRANSFORMACIONES NECESARIAS PARA PODER TENER UNA
VERSIÓN REFINADA DE ESOS ARCHIVOS.
          ¿A DÓNDE VAMOS?
NECESITAMOS ELEGIR PARA ESTOS ARCHIVOS UNA FORMA PARA PODER
CONSUMIRLOS. ARMAR UN MODELO PARA GUARDAR LOS DATOS QUE NOS
PERMITA CONSULTARLOS. ESTE MODELO DEBE SER ADECUADO PARA LOS
DATOS QUE TENEMOS Y PARA LAS NECESIDADES DE NUESTRA
ORGANZIACIÓN. PARA ESTO PODEMOS USAR LOS ARCHIVOS ACTUALES O
PODEMOS GENERAR NUEVOS ARCHIVOS, DEPENDIENDO DEL MODELO QUE
VAYAMOS A ELEGIR.
          ¿A DÓNDE VAMOS?
NECESITAMOS ELEGIR PARA ESTOS ARCHIVOS UNA FORMA PARA PODER
CONSUMIRLOS. ARMAR UN MODELO PARA GUARDAR LOS DATOS QUE NOS
PERMITA CONSULTARLOS. ESTE MODELO DEBE SER ADECUADO PARA LOS
DATOS QUE TENEMOS Y PARA LAS NECESIDADES DE NUESTRA
ORGANZIACIÓN. PARA ESTO PODEMOS USAR LOS ARCHIVOS ACTUALES O
PODEMOS GENERAR NUEVOS ARCHIVOS, DEPENDIENDO DEL MODELO QUE
VAYAMOS A ELEGIR.
  ¿POR QUÉ ES IMPORTANTE MODELAR LOS
                         DATOS?
PORQUE AYUDA A ORGANIZAR Y ESTRUCTURAR LOS DATOS ALMACENADOS
PARA QUE SEAN MÁS FÁCILES DE ENTENDER, ANALIZAR Y UTILIZAR.

UN DATALAKE ES UN REPOSITORIO CENTRALIZADO DE DATOS, QUE PUEDE
CONTENER UNA GRAN CANTIDAD DE INFORMACIÓN EN DIFERENTES
FORMATOS Y DE DIFERENTES ORÍGENES. LA VARIEDAD Y CANTIDAD DE
DATOS ALMACENADOS EN UN DATALAKE PUEDE SER ABRUMADORA, LO QUE
HACE QUE SEA DIFÍCIL PARA LOS USUARIOS ENCONTRAR LOS DATOS QUE
NECESITAN Y COMPRENDER SU SIGNIFICADO.
  ¿POR QUÉ ES IMPORTANTE MODELAR LOS
                 DATOS?
EL MODELADO DE DATOS RESUELVE ESTE PROBLEMA AL DEFINIR UNA
ESTRUCTURA LÓGICA Y COHERENTE PARA LOS DATOS ALMACENADOS EN EL
DATALAKE. AL CREAR UN MODELO DE DATOS, LOS DATOS SE ORGANIZAN EN
TABLAS, LO QUE HACE QUE SEA MÁS FÁCIL PARA LOS USUARIOS BUSCAR,
ENTENDER Y UTILIZAR LOS DATOS.
  ¿POR QUÉ ES IMPORTANTE MODELAR LOS
                 DATOS?
ADEMÁS, EL MODELADO DE DATOS TAMBIÉN AYUDA A GARANTIZAR LA
CALIDAD Y CONSISTENCIA DE LOS DATOS ALMACENADOS EN EL DATALAKE. AL
DEFINIR REGLAS Y RESTRICCIONES DE INTEGRIDAD DE DATOS, SE PUEDEN
EVITAR ERRORES Y DUPLICACIONES EN LOS DATOS ALMACENADOS.
  ¿POR QUÉ ES IMPORTANTE MODELAR LOS
                 DATOS?
POR LO TANTO, EL MODELADO DE DATOS ES IMPORTANTE DENTRO DE UN
DATALAKE PORQUE AYUDA A ORGANIZAR, ESTRUCTURAR Y COMPRENDER LOS
DATOS ALMACENADOS, LO QUE A SU VEZ FACILITA SU USO Y MEJORA SU
CALIDAD Y CONSISTENCIA.
  ¿CÓMO MODELAMOS LOS DATOS?

DE LAS TÉCNICAS QUE VAMOS A MENCIONAR, SON LAS REGLAS DEL NEGOCIO
LAS QUE EN ÚLTIMA INSTANCIA VAN A DEFINIR CUÁL VAMOS A UTILIZAR.
PODEMOS USAR UNA, TODAS O ALGÚN HÍBRIDO. LO IMPORTANTE ES TENER
CLARO CUALES SON LAS NECESIDADES DE LA ORGANIZACIÓN, Y COMO LOS
DATOS AYUDAN A QUE ESTAS SE CUMPLAN.
           MODELO NORMALIZADO

ESTE MODELO PROPUESTO POR BILL INMON, EL PADRE DEL DATAWAREHOUSE
TRATA DE REPLICAR LA FUENTE DE DATOS ORIGINAL DENTRO DEL DATALAKE,
MANTENIENDO LA NORMALIZACIÓN DE LOS DATOS. A PARTIR DE LA RÉPLICA
DE LAS FUENTES DE DATOS, SE CONSTRUYEN DATAMARTS QUE SON LUEGO
CONSULTADOS POR LA CAPA ANALÍTICA.
                      DATAMART
UN DATAMART ES ES UNA SUBSECCIÓN DE UN DATA WAREHOUSE QUE SE
ENFOCA EN UN ÁREA ESPECÍFICA DE UNA ORGANIZACIÓN, COMO LAS
VENTAS, EL MARKETING O LAS FINANZAS. TAMBIÉN PUEDE ENFOCARSE EN
UNA FUNCIÓN DENTRO DE LA ORGNZIACIÓN. SE UTILIZA PARA ALMACENAR
DATOS QUE SE HAN EXTRAÍDO, TRANSFORMADO Y CARGADO (ETL) DE
DIFERENTES FUENTES, Y SE HA ORGANIZADO DE TAL MANERA QUE LOS
USUARIOS PUEDAN ACCEDER A ELLOS DE MANERA FÁCIL Y RÁPIDA.
                       DATAMART
LOS DATAMARTS SUELEN ESTAR DISEÑADOS PARA SER UTILIZADOS POR UN
GRUPO ESPECÍFICO DE USUARIOS, COMO EL EQUIPO DE VENTAS, EL EQUIPO
DE MARKETING O EL EQUIPO DE FINANZAS. ESTO SIGNIFICA QUE LOS DATOS
SE PUEDEN PRESENTAR DE MANERA ESPECÍFICA PARA SATISFACER LAS
NECESIDADES DE LOS USUARIOS, LO QUE AUMENTA LA EFICACIA DE LOS
ANÁLISIS Y LOS INFORMES. SE PUEDEN UTILIZAR CON TODOS LOS MODELOS
DE DATOS QUE VAMOS A REPASAR.
           MODELO NORMALIZADO

ALGUNAS VENTAJAS SON QUE LOS USUARIOS FUERA DEL DATALAKE YA
CONOCEN EL MODELO DE DATOS DE LA FUENTE ORIGINAL Y AHORA LO
TIENEN DISPONIBLE EN EL DATALAKE JUNTO A OTRAS FUENTES DE DATOS
QUE LE PERMITEN ENRIQUECERLO. LOS DATOS SON UNA ÚNICA FUENTE DE
VERDAD, YA QUE ESTÁN IGUAL QUE LA FUENTE. LA REDUNDANCIA ES BAJA
PORQUE SE MANTIENE LA NORMALIZACIÓN.
           MODELO NORMALIZADO

ENTRE SUS DESVENTAJAS SE ENCUENTRA QUE ES DIFÍCIL LLEGAR A HACER
CONSULTAS SIN TENER QUE REALIZAR UNA GRAN CANTIDAD DE JOINS. Y
DIFÍCIL DE MANEJAR PARA QUIEN NO CONOCE LAS FUENTES ORIGINALES DE
DATOS.
           MODELO NORMALIZADO

POR MÁS QUE MANTENGAMOS LA NORMALIZACIÓN DE LOS DATOS. EL
DATALAKE NO ES UNA BASE DE DATOS TRANSACCIONAL. TIENE FINES
ANALÍTICOS. NO CUMPLE CON LAS PROPIEDADES ACID, QUE SON UNA SERIE
DE CARACTERÍSTICAS DE LAS TRANSACCIONES EN BASES DE DATOS
RELACIONALES TRADICIONALES.
           MODELO NORMALIZADO

POR MÁS QUE MANTENGAMOS LA NORMALIZACIÓN DE LOS DATOS. EL
DATALAKE NO ES UNA BASE DE DATOS TRANSACCIONAL. TIENE FINES
ANALÍTICOS. NO CUMPLE CON LAS PROPIEDADES ACID, QUE SON UNA SERIE
DE CARACTERÍSTICAS DE LAS TRANSACCIONES EN BASES DE DATOS
RELACIONALES TRADICIONALES.
                              ACID

ACID SIGNIFICA ATOMICIDAD, CONSISTENCIA, AISLAMIENTO Y DURABILIDAD, Y
ESTAS PROPIEDADES GARANTIZAN QUE LAS TRANSACCIONES SEAN
CONFIABLES Y SE COMPLETEN CORRECTAMENTE, INCLUSO EN PRESENCIA DE
FALLAS O CONFLICTOS.
                             ACID

ATOMICIDAD (ATOMICITY): ESTO SIGNIFICA QUE UNA TRANSACCIÓN SE
CONSIDERA UNA OPERACIÓN ÚNICA E INDIVISIBLE. EN OTRAS PALABRAS, UNA
TRANSACCIÓN SE EJECUTA POR COMPLETO O NO SE EJECUTA EN ABSOLUTO.
SI UNA PARTE DE LA TRANSACCIÓN FALLA, TODAS LAS OPERACIONES
REALIZADAS HASTA ESE PUNTO SE DESHACEN (SE REVIERTEN) PARA
MANTENER LA CONSISTENCIA DE LOS DATOS.
                            ACID

CONSISTENCIA (CONSISTENCY): LA CONSISTENCIA GARANTIZA QUE UNA
TRANSACCIÓN LLEVE LA BASE DE DATOS DESDE UN ESTADO VÁLIDO A OTRO
ESTADO VÁLIDO. EN OTRAS PALABRAS, LAS TRANSACCIONES DEBEN
MANTENER LA INTEGRIDAD DE LA BASE DE DATOS Y CUMPLIR CON TODAS LAS
RESTRICCIONES Y REGLAS DE NEGOCIO. SI UNA TRANSACCIÓN VIOLA
ALGUNA REGLA DE CONSISTENCIA, SE RECHAZA.
                            ACID

AISLAMIENTO (ISOLATION): LA PROPIEDAD DE AISLAMIENTO ASEGURA QUE
LAS TRANSACCIONES EN EJECUCIÓN SEAN INVISIBLES ENTRE SÍ HASTA QUE
SE COMPLETEN. ESTO EVITA QUE LAS TRANSACCIONES INTERFIERAN ENTRE
SÍ Y GARANTIZA QUE CADA TRANSACCIÓN VEA UNA VERSIÓN COHERENTE DE
LA BASE DE DATOS, COMO SI FUERA LA ÚNICA TRANSACCIÓN EN EJECUCIÓN.
                              ACID

DURABILIDAD (DURABILITY): LA DURABILIDAD SE REFIERE A LA CAPACIDAD DE
LA BASE DE DATOS PARA MANTENER LOS EFECTOS DE UNA TRANSACCIÓN
UNA VEZ QUE ESTA SE HA COMPLETADO CON ÉXITO. INCLUSO EN CASO DE
FALLAS EN EL SISTEMA, APAGONES DE ENERGÍA U OTROS PROBLEMAS, LOS
CAMBIOS REALIZADOS POR UNA TRANSACCIÓN EXITOSA DEBEN PERSISTIR Y
NO PERDERSE.
         CONSISTENCIA EVENTUAL


EN LUGAR DE UTILIZAR TRANSACCIONES ACID, LOS DATALAKES UTILIZAN
MODELOS DE CONSISTENCIA EVENTUAL O TRANSACCIONES DE BAJA
COHESIÓN PARA MANEJAR LOS DATOS.
         CONSISTENCIA EVENTUAL
LA CONSISTENCIA EVENTUAL SIGNIFICA QUE LOS DATOS PUEDEN TARDAR UN
TIEMPO EN PROPAGARSE A TRAVÉS DEL SISTEMA Y NO HAY GARANTÍA DE QUE
LOS USUARIOS VERÁN LOS MISMOS DATOS EN EL MISMO MOMENTO. LAS
TRANSACCIONES DE BAJA COHESIÓN PERMITEN QUE LAS OPERACIONES DE
ESCRITURA Y LECTURA SE REALICEN DE MANERA ASINCRÓNICA, LO QUE
PUEDE GENERAR UNA MAYOR LATENCIA PERO TAMBIÉN AUMENTA LA
ESCALABILIDAD DEL SISTEMA.
       MODELO DESNORMALIZADO

ESTE MODELO FUE DESARROLLADO POR RALPH KIMBALL. LOS DATOS
EXTRAÍDOS DE LA FUENTE DE DATOS, SON REFINADOS DE FORMA QUE
CREAMOS NUEVAS TABLAS CON LOS MISMOS. CREAMOS TABLAS DE HECHOS
CON AGREGACIONES Y TABLAS DIMENSIONALES, LO QUE CONOCEMOS COMO
DIAGRAMA ESTRELLA.
       MODELO DESNORMALIZADO

LAS TABLAS DE DIMENSIONES PUEDEN SER DE DOS TIPOS. QUE CONSERVEN
VALORES HISTÓRICOS, O QUE NO LOS CONSERVEN. AL ARMAR UNA TABLA DE
DIMENSIONES ASÍ NORMALMENTE UTILIZAMOS EL RANGO DE FECHA EN EL
QUE EL REGISTRO SE MANTUVO EN UN ESTADO Y LA FECHA DE CAMBIO
HASTA UNA FECHA INFINITA PARA EL NUEVO ESTADO. PODEMOS TAMBIÉN
USAR UN NÚMERO U OTRA REFERENCIA.
       MODELO DESNORMALIZADO

UNA DE SUS VENTAJAS ES QUE SIRVE PARA ANALISTAS DE DATOS QUE ESTÉN
ACOSTUMBRADOS A TRABAJAR CON ESQUEMAS MULTIDIMENSIONALES. SE
REQUIEREN MENOS JOINS PARA OBTENER LA INFORMACIÓN DESEADA POR LO
QUE LAS CONSULTAS SON MÁS RÁPIDAS QUE EL MODELO NORMALIZADO.
      MODELO DESNORMALIZADO


COMO DESVENTAJA PODEMOS SEÑALAR QUE HAY REDUNDANCIA EN LOS
DATOS Y ES MÁS COMPLEJO DE ARMAR Y MANTENER.
                     DATA VAULT


FUE PROPUESTO POR DAN LINSTEDT. ES UNA METODOLOGÍA DE MODELADO
DE DATOS QUE SE ENFOCA EN LA ESCALABILIDAD, LA FLEXIBILIDAD Y LA
AGILIDAD EN LA GESTIÓN DE GRANDES VOLÚMENES DE DATOS.
                      DATA VAULT

LOS DATOS SE ALMACENAN EN TABLAS BÁSICAS LLAMADAS "ENTIDADES" QUE
CONTIENEN INFORMACIÓN SOBRE LOS OBJETOS DEL MUNDO REAL, COMO
CLIENTES, PRODUCTOS O TRANSACCIONES. LUEGO, SE CREAN TABLAS
ADICIONALES LLAMADAS "RELACIONES" QUE SE UTILIZAN PARA VINCULAR
LAS ENTIDADES Y ALMACENAR INFORMACIÓN SOBRE LAS RELACIONES ENTRE
ELLAS.
                      DATA VAULT
LA ESTRUCTURA DEL DATA VAULT ESTÁ DISEÑADA PARA SER MODULAR Y
ESCALABLE, LO QUE PERMITE LA INCORPORACIÓN DE NUEVAS ENTIDADES Y
RELACIONES DE MANERA SENCILLA Y SIN AFECTAR LA ESTRUCTURA GENERAL
DEL MODELO. ADEMÁS, LA METODOLOGÍA DE DATA VAULT TAMBIÉN
INCORPORA TÉCNICAS PARA ASEGURAR LA INTEGRIDAD Y LA CALIDAD DE LOS
DATOS ALMACENADOS, ASÍ COMO LA CAPACIDAD DE RASTREAR EL ORIGEN Y
EL HISTORIAL DE LOS DATOS.
                      DATA VAULT

UNA DE LAS PRINCIPALES VENTAJAS DEL ENFOQUE DE DATA VAULT ES SU
CAPACIDAD PARA ADAPTARSE A LOS CAMBIOS EN LOS REQUISITOS DE DATOS
DE LA ORGANIZACIÓN, LO QUE LO HACE ESPECIALMENTE ADECUADO PARA
EMPRESAS CON DATOS COMPLEJOS Y EN CONSTANTE CAMBIO.
                      DATA VAULT


TAMBIÉN SE CONSIDERA UNA METODOLOGÍA DE ALMACÉN DE DATOS ÁGIL, LO
QUE SIGNIFICA QUE SE ENFOCA EN LA ENTREGA RÁPIDA DE INFORMACIÓN DE
ALTA CALIDAD A TRAVÉS DE ITERACIONES Y RETROALIMENTACIÓN CONTINUA.
                      DATA VAULT

ES EL MODELO MÁS COMPLEJO, MÁS DIFÍCIL DE IMPLEMENTAR Y MANTENER,
Y QUE MAYOR CAPACIDAD DE ALMACENAMIENTO REQUIERE, HACIÉNDOLO
UNO DE LOS MÁS COSTOSOS EN RECURSOS DE ESPACIO. TAMBIÉN SE
REQUIERE DE PERSONAL ALTAMENTE CAPACITADO PARA TRABAJAR CON ÉL.
                  ONE BIG TABLE

ES DE LOS MODELOS MÁS SIMPLES. CONSISTE EN TOMAR TODOS LOS DATOS
DE UN ORIGEN Y ARMAR CON ELLOS UNA GRAN TABLA, HACIENDO JOIN CON
TODAS LAS TABLAS, PARA CONSTRUIR UNA SÁBANA DE DATOS. ESTO
DESNORMALIZA TOTALMENTE LA BASE DE DATOS, Y SE OBTIENE UNA ALTA
REDUNDANCIA DE DATOS.
                  ONE BIG TABLE

ES DE LOS MODELOS MÁS SIMPLES. CONSISTE EN TOMAR TODOS LOS DATOS
DE UN ORIGEN Y ARMAR CON ELLOS UNA GRAN TABLA, HACIENDO JOIN CON
TODAS LAS TABLAS, PARA CONSTRUIR UNA SÁBANA DE DATOS. ESTO
DESNORMALIZA TOTALMENTE LA BASE DE DATOS, Y SE OBTIENE UNA ALTA
REDUNDANCIA DE DATOS.
                   ONE BIG TABLE

ES MUY SIMPLE DE CONSTRUIR Y PERMITE REALIZAR DATAMARTS
RÁPIDAMENTE CON UNOS POCOS JOINS. EL EQUIPO PUEDE ENFOCARSE
MUCHO MÁS EN LA LÓGICA DEL NEGOCIO Y DAR RESPUESTAS AL MISMO, QUE
ENFOCARSE EN EL MANTENIMIENTO DE LOS MODELOS.
                   ONE BIG TABLE

LA GRAN DESVENTAJA ES QUE A PESAR DE QUE EL ALMACENAMIENTO DE
TABLAS QUE SEAN MÁS AMPLIAS, CON VARIAS COLUMNAS ES BARATO Y LA
CAPACIDAD PARA CONSULTARLAS EXISTE. A MEDIDA QUE AUMENTEN LOS
DATOS SE HACE MÁS DIFÍCIL CONSULTARLOS, POR LO QUE A FUTURO ES UN
MODELO QUE VA A TENER PROBLEMAS PARA ESCALAR SI AUMENTA
MASIVAMENTE EL VOLÚMEN DE DATOS.
10 - HIVE
               ¿QUÉ ES HIVE?
SOFTWARE QUE FACILITA LA LECTURA, ESCRITURA Y MANEJO DE GRANDES
DATASETS RESIDENTES EN ALMACENAMIENTOS DISTRIBUIDOS, UTILIZANDO
SQL.

PERMITE PARTICIONES, TABLAS MANAGED/EXTERNAL, LAS CONSULTAS SE
TRANSFORMAN EN MAPREDUCE/TEZ
               ¿QUÉ ES HIVE?

HIVE SOPORTA
DATA DEFINITION LANGUAJE (DDL), DATA MANIPULATION LANGUAJE (DML) Y
USER DEFINITIONS FUNCTIONS (UDF).
          CARACTERÍSTICAS
HIVE ES RÁPIDO Y ESCALABLE. PROVEE DE CONSULTAS ESTILO SQL QUE
IMPLÍCITAMENTE SE TRANSFORMAN EN MAPREDUCE O SPARK JOBS.
ES CAPAZ DE ANALIZAR GRANDES VOLÚMENES DE DATOS ALMACENADOS EN
HDFS.
PERMITE DIFERENTES TIPOS DE ALMACENAMIENTOS COMO TEXTO PLANO,
ORC, PARQUET, ETC. PUEDE TRABAJAR SOBRE DATOS COMPRIMIDOS
ALMACENADOS DENTRO DE HDFS.
                 LIMITACIONES
HIVE NO ES CAPAZ DE MANEJAR DATOS         EN   TIEMPO   REAL.   SI   DE
ALMACENEROS NO PARA CONSULTARLOS.

HIVE NO ESTÁ DISEÑADO PARA PROCESAMIENTO DE TRANSACCIONES EN
LÍNEA.

LAS CONSULTAS HIVE CONTIENEN UNA ALTA LATENCIA.
  ARQUITECTURA (HIVE CLIENT)

HIVE PERMITE ESCRIBIR APLICACIONES EN VARIOS LENGUAJES, TALES COMO
PYTHON, JAVA, C#, ETC. POR ESO LA VARIEDAD DE DRIVERS QUE SOPORTA.
          ARQUITECTURA (HIVE
              SERVICES)
HIVE CLI: COMMAND LINE INTERFACE ES UN SHELL DONDE SE PUEDEN
EJECUTAR CONSULTAS HIVE Y COMANDOS.

HIVE WEB USER INTERFACE: ES UNA ALTERNATIVA A HIVE CLI, DONDE SE
PROVEE DE UNA INTERFAZ WEB PARA REALIZAR LAS MISMAS OPERACIONES.
          ARQUITECTURA (HIVE
HIVE
                   SERVICES)
     METASTORE: REPOSITORIO CENTRAL QUE ALMACENA         TODA   LA
INFORMACIÓN DE LAS TABLAS Y PARTICIONES. TAMBIÉN      INCLUYE   LA
METADATA DE LAS COLUMNAS Y SU INFORMACIÓN DE TIPOS.

HIVE SERVER: REFIERE A APACHE THRIFT SERVER Y ACEPTA EL PEDIDO DE
DIFERENTES CLIENTES Y PROVEEDORES AL HIVE DRIVER.
          ARQUITECTURA (HIVE
HIVE DRIVER: RECIBE CONSULTAS DESDE DIFERENTES FUENTES TALES COMO
              SERVICES)
UI, CLI, THRIFT, Y JDBC/ODBC DRIVER Y TRANSFIERE LAS CONSULTAS AL
COMPILADOR.

HIVE COMPILER: PARSEA LAS CONSULTAS Y REALIZA ANÁLISIS SEMÁNTICO DE
LOS DIFERENTES BLOQUES DE LA CONSULTA Y EXPRESIONES. ES EL
ENCARGADO DE CONVERTIR EL HIVEQL EN TRABAJOS MAP/REDUCE.

HIVE EXECUTION ENGINE: OPTIMIZA LOS PROCESOS MAPREDUCE Y SUS
ACCESOS A HDFS.
 TIPOS DE DATOS (NUMÉRICOS)
TINYINT     1 BYTE
SMALLINT      2 BYTE
INT     4   BYTE
BIGINT      8 BYTE
FLOAT       4 BYTE
DOUBLE        8 BYTE
   TIPOS DE DATOS (DATE TIME)
SOPORTA EL TRADICIONAL UNIX TIMESTAMP.
UN NÚMERO DE TIPO INTEGER, ES INTERPRETADO COMO UNIX TIMESTAMP EN
SEGUNDOS.

UN NÚMERO DEL TIPO FLOAT ES INTERPRETADO COMO UNIX TIMESTAMP EN
SEGUNDOS CON DOBLE PRECISIÓN.
EL RANGO DE LAS FECHAS VA ENTRE: 0000--01--01 A 9999--12--31.
 TIPOS DE DATOS (COMPLEJOS)

ARRAYS: CONTIENEN UNA LISTA DE ELEMENTOS DEL MISMO TIPO. ESTOS
ELEMENTOS SON ACCEDIDOS POR UN ÍNDICE. POR EJEMPLO, SI EL CAMPO
MASCOTA CONTIENE A LISTA DE ELEMENTOS [‘PERRO’, ’GATO’, ‘LORO’], EL
ELEMENTO ‘GATO’ PUEDE SER ACCEDIDO MEDIANTE MASCOTA[1].
 TIPOS DE DATOS (COMPLEJOS)

MAPS: CONTIENEN PARES CLAVE-VALOR. CADA ELEMENTO ES ACCEDIDO
MEDIANTE SU CLAVE. POR EJEMPLO, UN MAP LISTA_PASSWORDS
CONTENIENDO “MAURICIO” COMO CLAVE Y “PASSDEMAURICIO” COMO VALOR,
EL    PASSWORD     DEL     USUARIO    ES   ACCEDIDO     MEDIANTE
LISTA_PASSWORDS[‘MAURICIO’].
 TIPOS DE DATOS (COMPLEJOS)

STRUCTS: CONTIENEN ELEMENTOS DE DIFERENTE TIPOS. CADA ELEMENTOS
PUEDE SER ACCEDIDO MEDIANTE LA NOTACIÓN PUNTO (DOT NOTATION). POR
EJEMPLO, EN UNA ESTRUCTURA AUTO, EL COLOR DEL AUTO PUEDE SER
RECUPERADO MEDIANTE AUTO.COLOR.
     CREACIÓN BASE DE DATOS
EN HIVE UNA BASE DE DATOS ES CONSIDERADA COMO UN CATÁLOGO O
NAMESPACES DE TABLAS. HIVE PROVEE UNA BASE DE DATOS POR DEFECTO
CUYO NOMBRE ES DEFAULT.

PARA LISTAR LAS BASES DE DATOS EXISTENTES SOLO ES NECESARIO HACER:
  SHOW DATABASES;

PARA CREAR UNA NUEVA BASE DE DATOS SOLAMENTE SE DEBE HACER:
  CREATE DATABASE NOMBREBASEDATOS;
     CREACIÓN BASE DE DATOS
CADA BASE DE DATOS DEBE TENER UN NOMBRE ÚNICO.

SI NO TENEMOS CLARO SI UNA BASE DE DATOS DEL MISMO NOMBRE YA
EXISTE PODEMOS HACER: CREATE A DATABASE IF NOT EXISTS
NOMBREBASEDEDATOS.

HIVE PERMITE AL MOMENTO DE CREAR UNA BASE DE DATOS ASIGNAR
PROPIEDADES:

CREATE DATABASE NOMBREBASE
WITH DBPROPERTIES ('CREATOR' = 'USUARIO CREADOR', 'DATE' = '2019-06-
03');
  ELIMINACIÓN BASE DE DATOS
PARA ELIMINAR UNA BASE DE DATOS EXISTENTE SOLO SE DEBE HACER:
DROP DATABASE NOMBREBASE;

SI QUEREMOS ELIMINAR UNA BASE DE DATOS Y NO NOS DE ERROR SI LA
MISMA NO EXISTE, PODEMOS HACER:
DROP DATABASE IF EXISTS NOMBREBASE;
  ELIMINACIÓN BASE DE DATOS
EN HIVE NO ESTÁ PERMITIDO ELIMINAR UNA BASE DE DATOS QUE TENGA
TABLAS. PARA ELLO, PRIMERO SE DEBEN ELIMINAR LAS TABLAS Y LUEGO LA
BASE DE DATOS.

UNA ALTERNATIVA ES EL BORRADO EN CASCADA:
DROP DATABASE IF EXISTS CURSO
          CREACIÓN DE TABLAS
EN HIVE PODEMOS CREAR TABLAS UTILIZANDO CONVENCIONES SIMILARES A
SQL COMÚN.

EXISTEN DOS TIPOS DE TABLAS
INTERNAS
EXTERNAS
              TABLAS INTERNAS
PARA CREAR UNA TABLA INTERNA DEBEMOS HACER:

CREATE TABLE NOMBREBASE.NOMBREALUMNO (ID INT, NAME STRING , EDAD
INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',' ;

ESTE TIPO DE TABLAS SON GENERALMENTE ALMACENADAS EN          UN
SUBDIRECTORIO   DEBAJO   DEL   DIRECTORIO  DEFINIDO   POR    EL
HIVE.METASTORE.WAREHOUSE.DIR( POR EJ /USER/HIVE/WAREHOUSE)   EN
HDFS
              TABLAS INTERNAS
ES IMPORTANTE DESTACAR QUE SI ELIMINAMOS UNA TABLA INTERNA, NO
SOLO ELIMINAMOS LA TABLA EN SÍ MISMA, SI NO LOS DATOS ASOCIADOS A LA
MISMA, POR ESO HAY QUE TENER MUCHO CUIDADO AL HACERLO.

SI QUEREMOS CREAR UNA TABLA Y ASEGURARNOS DE QUE NO DE ERROR SI
YA EXISTE, SOLO LE AGREGAMOS: IF NOT EXISTS.
                  TABLAS INTERNAS
PARA INSERTAR DATOS DENTRO DE UNA TABLA INTERNA TENEMOS DOS
FORMAS:

INSERT INTO TABLE <TABLE_NAME> VALUES (<ADD VALUES AS PER COLUMN
ENTITY>);

INSERT INTO TABLE         STUDENT   VALUES   ('DIKSHANT',1,'95'),('AKSHAT',   2   ,
'96'),('DHRUV',3,'90');
               TABLAS INTERNAS
HIVE TAMBIÉN NOS DA LA POSIBILIDAD DE CARGAR DATOS A PARTIR DE
ARCHIVOS O DATOS PRECARGADOS, YA SEAN LOCALES O EN HDFS,
MEDIANTE LOAD DATA.

LOAD DATA [LOCAL] INPATH '<THE TABLE DATA LOCATION>' [OVERWRITE] INTO
TABLE <TABLE_NAME>;

LOAD DATA LOCAL INPATH '/HOME/DIKSHANT/DOCUMENTS/DATA.CSV' INTO
TABLE STUDENT;
             TABLAS EXTERNAS
LAS TABLAS EXTERNAS NOS PERMITEN CREAR Y ACCEDER A LA TABLA Y LOS
DATOS EXTERNAMENTE. EN ESTE TIPO DE TABLAS, LOS DATOS NO SON
ALMACENADOS EN EL DIRECTORIO HIVE, POR LO QUE SI ELIMINAMOS UNA
TABLA DE ESTE TIPO, SOLO ELIMINAMOS LA TABLA Y NO LOS DATOS
ASOCIADOS.

PARA CREAR UNA TABLA EXTERNA PRIMERO QUE NADA DEBEMOS CREAR EL
DIRECTORIO HDFS DONDE SE VA A ALMACENAR LA INFORMACIÓN.
              TABLAS EXTERNAS
PARA ESO HACEMOS COMO VIMOS EN CLASES ANTERIORES:
HDFS DFS -MKDIR /HIVEDIRECTORY

LUEGO, ALMACENAMOS LA INFORMACIÓN QUE QUERAMOS         EN   ESE
DIRECTORIO, VÍA LA HERRAMIENTA QUE SE QUIERA POR EJ:

HDFS DFS -PUT HIVE/EMP_DETAILS /HIVEDIRECTORY
              TABLAS EXTERNAS
LUEGO, CREAMOS LA TABLA DE LA SIGUIENTE FORMA:

CREATE EXTERNAL TABLE CURSO.ALUMNOS (ID INT, NAME STRING , CI INT)
ROW FORMAT DELIMITED
 FIELDS TERMINATED BY ','
LOCATION '/HIVEDIRECTORY';

LUEGO PUEDO HACER:
SELECT * FROM NOMBREBASE.NOMBRETABLA
             TABLAS EXTERNAS
LA FORMA DE CARGAR DATOS ES SIMILAR A UNA TABLA INTERNA VÍA LOAD
DATA O INSERT INTO. LA DIFERENCIA RADICA EN DONDE SE ALMACENAN LOS
DATOS.

UNA VEZ DEFINIDO EL LUGAR EN HDFS DONDE SE ALMACENAN LOS DATOS, SI
SE CARGAN NUEVOS ARCHIVOS A ESE DIRECTORIO LA TABLA VA A TOMAR
ESOS DATOS EN FORMA AUTOMÁTICA.
              TABLAS EXTERNAS
LOS DATOS DE LAS TABLAS CREADAS EN HIVE PUEDEN SER DE DIFERENTES
TIPOS: CSV, ORC, PARQUET, ETC. PARA ELLO AL CREAR LA TABLA SE DEBE
ESPECIFICAR EL TIPO (SI NO SE QUIERE EL POR DEFECTO QUE ES ORC).

CREATE EXTERNAL TABLE MYTABLE
(COL1 BIGINT,COL2 BIGINT) ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS ORC
LOCATION '<ORC FILE LOCATION'
               ELIMINAR TABLAS
HIVE OFRECE LA POSIBILIDAD DE ELIMINAR TABLAS ANTERIORMENTE
CREADAS (YA SEAN INTERNAS O EXTERNAS) MEDIANTE EL COMANDO DROP.

DROP TABLE NOMBREBASE.NOMBRETABLA;

RECUERDEN QUE SI ELIMINAN UNA TABLA INTERNA, LA DATA CONTENIDA EN
LA MISMA ES ELIMINADA TAMBIÉN, MIENTRAS QUE EN UNA TABLA EXTERNA, LA
DATA CONTINÚA SIN PROBLEMAS.
            MODIFICAR TABLAS

EN HIVE PODEMOS HACER MODIFICACIONES, ACTUALIZACIONES EN TABLAS
EXISTENTES COMO CAMBIAR EL NOMBRE DE LA TABLA, EL NOMBRE DE
ALGUNA COLUMNA, AGREGAR PROPIEDADES, ETC.
                   RENOMBRAR

SI QUEREMOS CAMBIAR EL NOMBRE DE UNA TABLA HACEMOS:

ALTER TABLE OLD_TABLE_NAME RENAME TO NEW_TABLE_NAME;
            ELIMINAR COLUMNAS
HIVE NOS DA LA POSIBILIDAD DE ELIMINAR COLUMNAS DE TABLAS YA
EXISTENTES VÍA REPLACE. SE TIENE ESTA TABLA DE NOMBRE DATOS QUE
TIENE: ID (INT), NAME (STRING), SALARY(FLOAT), AGE(INT) Y SE HACE:

ALTER TABLE DATOS REPLACE COLUMNS( ID STRING, FIRST_NAME STRING,
AGE INT);

LA NUEVA TABLA TENDRÁ:
ID (INT), NAME (STRING), SALARY(FLOAT), AGE(INT)
             PARTICIONAMIENTO

EL PARTICIONAMIENTO EN HIVE SIGNIFICA DIVIDIR LOS DATOS CONTENIDOS
EN UNA TABLA EN VARIAS PARTES DE ACUERDO A UNA COLUMNA PARTICULAR.

PERO, ¿POR QUÉ ES ÚTIL ESTO?
            PARTICIONAMIENTO
HADOOP ES UTILIZADO PARA MANEJAR GRANDES VOLÚMENES DE DATOS,
POR LO QUE SIEMPRE REQUIERE LA MEJOR MANERA DE TRABAJAR CON LOS
MISMOS. EL PARTICIONAMIENTO ES UNA DE ELLAS.
SUPONGAMOS QUE TENGAMOS UNA TABLA CON 10 MILLONES DE REGISTROS
QUE CONTIENE DATOS DE ESTUDIANTES Y QUEREMOS SABER QUÉ
ESTUDIANTES CURSAN UN CURSO DETERMINADO.
SI CONTAMOS CON UNA COLUMNA CURSO Y PARTICIONAMOS POR ELLA,
CUANDO VAMOS A BUSCAR DETERMINADO CURSO, IRÍAMOS A ESA
INFORMACIÓN.
             PARTICIONAMIENTO

EXISTEN DOS TIPOS DE PARTICIONAMIENTO:

ESTÁTICO Y DINÁMICO
  PARTICIONAMIENTO ESTÁTICO
SE CONFIGURA AL MOMENTO DE DEFINIR LA TABLA Y ES REQUERIDO PASAR
LOS VALORES DE LAS COLUMNAS PARTICIONADAS MIENTRAS SE CARGAN LOS
DATOS A LA TABLA.

CREATE TABLE STUDENT (ID INT, NAME STRING, AGE INT, INSTITUTE STRING)
PARTITIONED BY (COURSE STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
 PARTICIONAMIENTO DINÁMICO
EN EL CASO DEL PARTICIONAMIENTO DINÁMICO, NO ES NECESARIO PASAR
LOS VALORES DE LAS COLUMNAS PARTICIONADAS. CREAMOS UNA TABLA:
CREATE TABLE STUD_DEMO(ID INT, NAME STRING, AGE INT, INSTITUTE
STRING, COURSE STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
LUEGO, PERMITIMOS EL PARTICIONAMIENTO DINÁMICO VIA:
HIVE> SET HIVE.EXEC.DYNAMIC.PARTITION=TRUE;
HIVE> SET HIVE.EXEC.DYNAMIC.PARTITION.MODE=NONSTRICT;
 PARTICIONAMIENTO DINÁMICO
EN EL CASO DEL PARTICIONAMIENTO DINÁMICO, NO ES NECESARIO PASAR
LOS VALORES DE LAS COLUMNAS PARTICIONADAS. CREAMOS UNA TABLA:
CREATE TABLE STUD_DEMO(ID INT, NAME STRING, AGE INT, INSTITUTE
STRING, COURSE STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
LUEGO, PERMITIMOS EL PARTICIONAMIENTO DINÁMICO VIA:
HIVE> SET HIVE.EXEC.DYNAMIC.PARTITION=TRUE;
HIVE> SET HIVE.EXEC.DYNAMIC.PARTITION.MODE=NONSTRICT;
  PARTICIONAMIENTO DINÁMICO
LA TABLA HASTA AHORA NO TIENE PARTICIÓN PERO SI HACEMOS UNA TABLA
DUMMY DEL TIPO:
CREATE TABLE STUDENT_PART (ID INT, NAME STRING, AGE INT, INSTITUTE
STRING)
PARTITIONED BY (COURSE STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
  PARTICIONAMIENTO DINÁMICO
LUEGO INSERTAMOS DATOS:

INSERT INTO STUDENT_PART
PARTITION(COURSE)
SELECT ID, NAME, AGE, INSTITUTE, COURSE
FROM STUD_DEMO;

ESTO NOS PERMITE IR CREANDO NUEVAS PARTICIONES POR COLUMNAS A
MEDIDA INSERTAMOS DATOS.
    OPERADORES ARITMÉTICOS
A + B SUMA COLUMNA A + B

A - B RESTA COLUMA A - B

A * B MULTIPLICA COLUMNA A * B

A / B DIVIDE A SOBRE B

A % B RETORNA EL RESTO DE A SOBRE B
  OPERADORES RELACIONALES
A=B TRUE SI A = B FALSE EN CASO CONTRARIO

A <> B, A !=B NULL SI A O B SON NULOS. TRUE SI A <> B FALSE EN CASO
CONTRARIO

A<B NULL SI A O B SON NULOS. TRUE SI A MENOR QUE B FALSE EN CASO
CONTRARIO
     FUNCIONES MATEMÁTICAS
ROUND(NUM) REDONDEA EL NÚMERO DADO.

FLOOR(NUM) RETORNA EL ENTERO MENOR O IGUAL AL NÚMERO.

EXP(NUM) RETORNA EL EXPONENCIAL DEL NÚMERO.

LN(NUM) RETORNA EL LOGARTIMO NATURAL DEL NÚMERO.
     FUNCIONES MATEMÁTICAS

LOG10(NUM) RETORNA EL LOGARITMO BASE 10 DEL NÚMERO.

SQRT(NUM) RAÍZ CUADRADA DEL NÚMERO.

ABS(NUM) VALOR ABSOLUTO DEL NÚMERO
            FUNCIONES BUILT-IN
LENGTH(STR) LARGO DEL STRING

REVERSE(STR) RETORNA EL STRING EN FORMA INVERSA.

CONCAT(STR1, STR2, ...) CONCATENA LOS STRINGS.

SUBSTR(STR, START_INDEX) RETORNA EL SUBSTRING DE STR COMENZANDO
EN START_INDEX.
           FUNCIONES BUILT-IN
SUBSTR(STR, INT START, INT LENGTH) RETORNA EL SUBSTRING DE STR
COMENZANDO EN START Y DE LARGO LENGHT.

SQRT(NUM) RAÍZ CUADRADA DEL NÚMERO..

ABS(NUM) VALOR ABSOLUTO DEL NÚMERO.
           GOBIERNO DE DATOS
AL IGUAL DE LO QUE HABLAMOS CON HDFS EL GOBIERNO ES FUNDAMENTAL
DENTRO DE HIVE. BÁSICAMENTE DEBERÍAMOS TENER ESQUEMAS PARA
DESARROLLO Y ESQUEMAS PARA DEJAR EN PRODUCCIÓN.

ORGANIZAR LOS DATOS EN MODELOS EN DISTINTAS TABLAS COLABORA CON
EL GOBIERNO DE DATOS, EL CATALOGADO DE LOS MISMOS, LA TRAZABILIDAD
Y LA INTEGRIDAD. LOGRAMOS UNA ÚNICA REALIDAD CON LOS DATOS QUE ES
TRANSVERSAL A LA ORGANIZACIÓN.
           GOBIERNO DE DATOS
ES RECOMENDABLE QUE NO TODOS TENGAN PERMISO PARA CREAR TABLAS O
ACCEDER A LOS DATOS QUE HAY EN ELLAS. HAY DIFERENTES HERRAMIENTAS
COMO POR EJEMPLO APACHE RANGER QUE NOS PERMITEN GESTIONAR LOS
PERMISOS PARA CREAR TABLAS TANTO EN ESQUEMAS DE DESARROLLO
COMO DE PRODUCCIÓN, ASÍ COMO PARA ACCEDER A LOS MISMOS. RANGER
TAMBIÉN CONTROLA EL ACCESO A TODOS LOS DIRECTORIOS QUE CREEMOS
EN HDFS.
